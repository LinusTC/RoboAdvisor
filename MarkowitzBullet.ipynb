{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj91ThpmabDR"
   },
   "source": [
    "# Investment Robo-advisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w31yow0TabDS"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvpNgOhEabDS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout # type: ignore\n",
    "\n",
    "from fetchData import fetch_raw_data_yf, getSNP500, fetch_raw_data_yf_all, getNasdaq_comp, add_days_to_date\n",
    "from MonteCarloRBA import MonteCarloRBA\n",
    "from PortfolioFunction import maximize_sharpe, create_correlation_matrix, get_sharpe_ratio, get_matrices, maximize_sharpe_SLSQP\n",
    "from LearningRBA import find_best_asset_to_remove, find_asset_to_add\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnySmmT6abDT"
   },
   "source": [
    "## 2. Fetch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bz1yuGUabDT"
   },
   "source": [
    "### Get all Nasdaq Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4G7yoJuabDT",
    "outputId": "e6e2de66-44f1-4d77-8bc1-bfe43fe6e4cd"
   },
   "outputs": [],
   "source": [
    "assets= [\n",
    "    \"AAPL\",  # Apple Inc.\n",
    "    \"MSFT\",  # Microsoft Corporation\n",
    "    \"AMZN\",  # Amazon.com Inc.\n",
    "    \"GOOGL\", # Alphabet Inc. (Google) Class A\n",
    "    \"GOOG\",  # Alphabet Inc. (Google) Class C\n",
    "    \"META\",    # Meta Platforms Inc (formerly Facebook)\n",
    "    \"TSLA\",  # Tesla Inc\n",
    "    \"UA\", # Berkshire Hathaway Inc. Class B\n",
    "    \"JPM\",   # JPMorgan Chase & Co.\n",
    "    \"V\",     # Visa Inc.\n",
    "    \"JNJ\",   # Johnson & Johnson\n",
    "    \"WMT\",   # Walmart Inc.\n",
    "    \"PG\",    # Procter & Gamble Co.\n",
    "    \"UNH\",   # UnitedHealth Group Inc.\n",
    "    \"MA\",    # Mastercard Inc.\n",
    "    \"NVDA\",  # NVIDIA Corporation\n",
    "    \"HD\",    # Home Depot Inc.\n",
    "    \"BAC\",   # Bank of America Corp\n",
    "    \"DIS\",   # Walt Disney Co\n",
    "    \"PYPL\",  # PayPal Holdings\n",
    "    \"VZ\",    # Verizon Communications Inc.\n",
    "    \"ADBE\",  # Adobe Inc.\n",
    "    \"CMCSA\", # Comcast Corporation\n",
    "    \"NFLX\",  # Netflix Inc.\n",
    "    \"KO\",    # Coca-Cola Co\n",
    "    \"NKE\",   # NIKE Inc.\n",
    "    \"PFE\",   # Pfizer Inc.\n",
    "    \"MRK\",   # Merck & Co., Inc.\n",
    "    \"PEP\",   # PepsiCo, Inc.\n",
    "    \"T\",     # AT&T Inc.\n",
    "    \"ABT\",   # Abbott Laboratories\n",
    "    \"CRM\",   # Salesforce.com Inc.\n",
    "    \"ORCL\",  # Oracle Corporation\n",
    "    \"ABBV\",  # AbbVie Inc.\n",
    "    \"CSCO\",  # Cisco Systems, Inc.\n",
    "    \"INTC\",  # Intel Corporation\n",
    "    \"TMO\",   # Thermo Fisher Scientific Inc.\n",
    "    \"XOM\",   # Exxon Mobil Corporation\n",
    "    \"ACN\",   # Accenture plc\n",
    "    \"LLY\",   # Eli Lilly and Company\n",
    "    \"COST\",  # Costco Wholesale Corporation\n",
    "    \"MCD\",   # McDonald's Corp\n",
    "    \"DHR\",   # Danaher Corporation\n",
    "    \"MDT\",   # Medtronic plc\n",
    "    \"NEE\",   # NextEra Energy, Inc.\n",
    "    \"BMY\",   # Bristol-Myers Squibb Company\n",
    "    \"QCOM\",  # Qualcomm Inc\n",
    "    \"CVX\",   # Chevron Corporation\n",
    "    \"WFC\",   # Wells Fargo & Co\n",
    "    \"LMT\",    # Lockheed Martin Corporation\n",
    "    \"GS\",   # Goldman Sachs Group, Inc.\n",
    "    \"MS\",   # Morgan Stanley\n",
    "    \"IBM\",  # International Business Machines Corporation\n",
    "    \"GE\",   # General Electric Company\n",
    "    \"F\",    # Ford Motor Company\n",
    "    \"GM\",   # General Motors Company\n",
    "    \"UBER\", # Uber Technologies, Inc.\n",
    "    \"LYFT\", # Lyft, Inc.\n",
    "    \"SNAP\", # Snap Inc.\n",
    "    \"TWTR\", # Twitter, Inc.\n",
    "    \"SPOT\", # Spotify Technology S.A.\n",
    "    \"AMD\",  # Advanced Micro Devices, Inc.\n",
    "    \"TXN\",  # Texas Instruments Incorporated\n",
    "    \"BABA\", # Alibaba Group Holding Limited\n",
    "    \"SAP\",  # SAP SE\n",
    "    \"HON\",  # Honeywell International Inc.\n",
    "    \"BA\",   # Boeing Company\n",
    "    \"RTX\",  # Raytheon Technologies Corporation\n",
    "    \"CAT\",  # Caterpillar Inc.\n",
    "    \"DE\",   # Deere & Company\n",
    "    \"MMM\",  # 3M Company\n",
    "    \"DUK\",  # Duke Energy Corporation\n",
    "    \"SO\",   # Southern Company\n",
    "    \"EXC\",  # Exelon Corporation\n",
    "    \"NEE\",  # NextEra Energy, Inc.\n",
    "    \"AEP\",  # American Electric Power Company, Inc.\n",
    "    \"SRE\",  # Sempra Energy\n",
    "    \"ETN\",  # Eaton Corporation plc\n",
    "    \"EMR\",  # Emerson Electric Co.\n",
    "    \"SYY\",  # Sysco Corporation\n",
    "    \"KR\",   # Kroger Co.\n",
    "    \"GIS\",  # General Mills, Inc.\n",
    "    \"K\",    # Kellogg Company\n",
    "    \"CPB\",  # Campbell Soup Company\n",
    "    \"MO\",   # Altria Group, Inc.\n",
    "    \"PM\",   # Philip Morris International Inc.\n",
    "    \"BTI\",  # British American Tobacco plc\n",
    "    \"RDY\",  # Dr. Reddy's Laboratories Ltd.\n",
    "    \"GILD\", # Gilead Sciences, Inc.\n",
    "    \"BIIB\", # Biogen Inc.\n",
    "    \"CELG\", # Celgene Corporation\n",
    "    \"AMGN\", # Amgen Inc.\n",
    "    \"SYK\",  # Stryker Corporation\n",
    "    \"BSX\",  # Boston Scientific Corporation\n",
    "    \"ISRG\", # Intuitive Surgical, Inc.\n",
    "    \"ZBH\",  # Zimmer Biomet Holdings, Inc.\n",
    "    \"EW\",   # Edwards Lifesciences Corporation\n",
    "    \"RMD\",  # ResMed Inc.\n",
    "    \"VRTX\", # Vertex Pharmaceuticals Incorporated\n",
    "    \"REGN\",  # Regeneron Pharmaceuticals, Inc.\n",
    "]\n",
    "\n",
    "assets = getSNP500()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvJfF_5YabDU",
    "outputId": "e2769ab4-6379-4ccc-e628-557b92733cd1"
   },
   "outputs": [],
   "source": [
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2018-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, asset_errors, max_combination= fetch_raw_data_yf(assets, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyXEENSbabDU"
   },
   "source": [
    "## 3. Mean, Volatility and Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8UnmVllabDU",
    "outputId": "5db2e605-2732-40fd-8052-5cddc38fded9"
   },
   "outputs": [],
   "source": [
    "names, annualized_returns, unweighted_annaulized_returns, weighted_returns_matrix, normal_returns_matrix, cov, correlation_matrix = get_matrices(raw_data)\n",
    "\n",
    "volatility = np.sqrt(np.diag(cov))\n",
    "risk_free_rate=0\n",
    "sharpe_ratios = (annualized_returns - risk_free_rate) / volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "70g4vm3qabDU",
    "outputId": "3974e87c-1b85-4392-e8b8-594c2626466e"
   },
   "outputs": [],
   "source": [
    "hover_texts = [\n",
    "    f\"<br>Symbol: {ticker} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
    "    for ticker, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
    "]\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=volatility,\n",
    "    y=annualized_returns,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    hovertext=hover_texts,\n",
    "    marker=dict(color=sharpe_ratios, colorscale = 'RdBu', size=6, line=dict(width=1), colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Annual Performance of Individual Assets',\n",
    "    xaxis_title='Volatility (Standard Deviation)',\n",
    "    yaxis_title='Annualized Returns',\n",
    "    width = 1920,\n",
    "    height = 1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"PerformanceofIndividualAssets.html\")\n",
    "#fig.write_image(\"PerformanceofIndividualAssets.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating SLSQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_assets = np.random.choice(list(names), 5, replace=False)\n",
    "\n",
    "selected_returns = annualized_returns.loc[rand_assets].values\n",
    "selected_covariances = cov.loc[rand_assets, rand_assets].values\n",
    "\n",
    "optimized_weights, weights_history, sharpe_history = maximize_sharpe_SLSQP(selected_returns, selected_covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Portfolio Weights Over Iterations\", \"Sharpe Ratio Over Iterations\"))\n",
    "\n",
    "for i in range(len(selected_returns)):\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(weights_history))), \n",
    "                             y=[h[i] for h in weights_history], \n",
    "                             mode='lines+markers', \n",
    "                             name=f'Asset {i+1} Weight'), \n",
    "                  row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(sharpe_history))), \n",
    "                         y=sharpe_history, \n",
    "                         mode='lines+markers', \n",
    "                         name='Sharpe Ratio'), \n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_layout(title_text='Portfolio Optimization Analysis',\n",
    "                  xaxis_title='Iteration',\n",
    "                  yaxis_title='Weight',\n",
    "                  legend_title='Assets',\n",
    "                    font=dict(\n",
    "                        family=\"Cambria\",\n",
    "                        size=18,\n",
    "                    )\n",
    ")\n",
    "\n",
    "# Update xaxis and yaxis properties for Sharpe Ratio subplot\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sharpe Ratio\", row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#fig.write_html(\"SLSQPDemo.html\")\n",
    "#fig.write_image(\"SLSQPDemo.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3i7iFfDabDV"
   },
   "source": [
    "## 4.0 Monte Carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsgRqCSWabDV",
    "outputId": "55346305-b21e-48aa-bd8b-0daed1a50060"
   },
   "outputs": [],
   "source": [
    "all_portfolios, dominant_portfolios = MonteCarloRBA(names, cov, annualized_returns, 10000, min_assets=8, max_assets=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09tYBpwxabDV",
    "outputId": "055bc292-d45d-4b5b-baaf-482d1c4ede81"
   },
   "outputs": [],
   "source": [
    "iterations = [portfolio['iteration'] for portfolio in dominant_portfolios]\n",
    "counts = list(range(1, len(dominant_portfolios) + 1))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=iterations, y=counts,\n",
    "                        mode='lines',\n",
    "                        name='Dominant Portfolios',\n",
    "                        line=dict(shape='spline')\n",
    "))  \n",
    "\n",
    "fig.update_layout(\n",
    "    title='Growth of Dominant Portfolios Over Iterations',\n",
    "    xaxis_title='Portfolios Generated',\n",
    "    yaxis_title='Number of Dominant Portfolios Found',\n",
    "    height=1080,\n",
    "    width=1920,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"FrequencyOfDom.html\")\n",
    "#fig.write_image(\"FrequencyOfDom.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "uuj_oTIZabDV",
    "outputId": "c0fec4f3-4492-4608-a54f-48d800b04ba7"
   },
   "outputs": [],
   "source": [
    "fig1 = go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=[np.sqrt(p[\"variance\"]) for p in all_portfolios],\n",
    "    y=[p[\"return\"] for p in all_portfolios],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=[p[\"sharpe\"] for p in all_portfolios],\n",
    "        showscale=True,\n",
    "        size=7,\n",
    "        line=dict(width=1),\n",
    "        colorscale=\"RdBu\",\n",
    "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[\n",
    "        f\"Return: {p['return']:.3%}<br>Volatility: {np.sqrt(p['variance']):.3f}<br>\" +\n",
    "        f\"Sharpe Ratio: {p['return'] / (np.sqrt(p['variance'])):.3f}<br>\" +\n",
    "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
    "        for p in all_portfolios\n",
    "    ]\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    xaxis=dict(title='Volatility (Standard Deviation)'),\n",
    "    yaxis=dict(title='Annualised Returns'),\n",
    "    title='Monte Carlo Randomly Generated Portfolios',\n",
    "    height=1080,\n",
    "    width=1920,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "fig1.show()\n",
    "#fig1.write_html(\"MonteCarlo.html\")\n",
    "#fig1.write_image(\"MonteCarlo.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "uHkH6APvabDV",
    "outputId": "8dbda4b6-b409-48de-98ea-c89f8e6f5f85"
   },
   "outputs": [],
   "source": [
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=[np.sqrt(p[\"variance\"]) for p in dominant_portfolios],  # Convert variance to volatility\n",
    "    y=[p[\"return\"] for p in dominant_portfolios],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=7,\n",
    "        line=dict(width=1),\n",
    "        #showscale=True,\n",
    "        #color=[p[\"return\"] / (np.sqrt(p[\"variance\"])) for p in dominant_portfolios],  # Sharpe Ratio\n",
    "        #colorscale=\"RdBu\",\n",
    "        #colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[\n",
    "        f\"Return: {p['return']:.3%}<br>Volatility: {np.sqrt(p['variance']):.3f}<br>\" +\n",
    "        f\"Sharpe Ratio: {p['sharpe']:.3f}<br>\" +\n",
    "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
    "        for p in dominant_portfolios\n",
    "    ],\n",
    "    name=\"Monte Carlo Portfolios\"\n",
    "))\n",
    "\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=volatility,\n",
    "    y=annualized_returns,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    hovertext=[\n",
    "        f\"{name} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
    "        for name, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
    "    ],\n",
    "    marker=dict(\n",
    "        color='green',\n",
    "        size=5,\n",
    "        line=dict(width=1)\n",
    "    ),\n",
    "    name=\"Individual Assets\"\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='Monte Carlo Portfolios with Individual Assets',\n",
    "    xaxis_title='Volatility (Standard Deviation)',\n",
    "    yaxis_title='Annualized Return',\n",
    "    legend=dict(y=5),\n",
    "    height=1080,\n",
    "    width=1920,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "#fig2.write_html(\"MCMarkowitzBullet.html\")\n",
    "#fig2.write_image(\"MCMarkowitzBullet.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-WDtJTgabDV"
   },
   "source": [
    "## 5.0 Machine Learning Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Optimization Function Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xbdm-4NabDV",
    "outputId": "53c0d514-7544-4e8f-9ed5-4af9aa786a56"
   },
   "outputs": [],
   "source": [
    "def MLRBA_V1(ticker, covariances, returns, num_iterations=None, risk_free_rate = 0, \n",
    "             return_power = 1, std_power = 1, return_weight=1/3, corr_weight=1/3, vol_weight= 1/3, num_assets = 8, base_portfolio = None):\n",
    "    \n",
    "    if num_iterations is None:\n",
    "        num_iterations = min(math.comb(len(ticker), num_assets), 100000)\n",
    "    \n",
    "    if base_portfolio is None:\n",
    "        base_portfolio = np.random.choice(list(ticker), num_assets, replace=False)\n",
    "        #base_portfolio = list(ticker)[:num_assets]\n",
    "    \n",
    "    def _get_portfolio_stats (portfolio_assets, risk_free_rate = 0):\n",
    "        p_asset_ret = returns.loc[portfolio_assets].values\n",
    "        p_asset_var = covariances.loc[portfolio_assets, portfolio_assets].values\n",
    "        best_p_weights = maximize_sharpe(p_asset_ret, p_asset_var)\n",
    "        p_ret = np.dot(best_p_weights,p_asset_ret)\n",
    "        p_var = np.dot(best_p_weights, p_asset_var @ best_p_weights)\n",
    "        sharpe = get_sharpe_ratio(p_ret, p_var, risk_free_rate, return_power, std_power)\n",
    "\n",
    "        return p_asset_ret, p_asset_var, sharpe, p_ret, p_var, best_p_weights\n",
    "\n",
    "    def _update_portfolios_array(portfolios, assets, weights, p_ret, p_var):\n",
    "        portfolios.append({\n",
    "            \"tickers\": assets,\n",
    "            \"weights\": weights,\n",
    "            \"return\": p_ret,\n",
    "            \"variance\": p_var,\n",
    "            \"sharpe\": (p_ret-risk_free_rate)/np.sqrt(p_var),\n",
    "        })\n",
    "\n",
    "    all_portfolios = []\n",
    "    \n",
    "    curr_ret, curr_var, curr_weighted_sharpe, curr_p_return, curr_p_variance, curr_p_weights = _get_portfolio_stats(base_portfolio, risk_free_rate)\n",
    "    _update_portfolios_array(all_portfolios, base_portfolio, curr_p_weights, curr_p_return, curr_p_variance)\n",
    "\n",
    "    good_portfolios = all_portfolios.copy()\n",
    "    best_portfolio = base_portfolio.copy()\n",
    "\n",
    "    highest_weighted_sharpe = -np.inf\n",
    "    highest_weighted_sharpe = curr_weighted_sharpe\n",
    "    \n",
    "    portfolios_tested = 0\n",
    "    best_iteration = 0\n",
    "\n",
    "    progress_bar = tqdm(total=num_iterations, desc=\"Portfolios Tested\")\n",
    "    for _ in range(num_iterations):\n",
    "        asset_to_remove = find_best_asset_to_remove(best_portfolio, curr_var, curr_ret)\n",
    "        new_portfolio = [str(asset) for asset in best_portfolio if asset != asset_to_remove]\n",
    "\n",
    "        ranked_assets = find_asset_to_add(new_portfolio, ticker, covariances, returns,\n",
    "                                          return_weight, corr_weight, vol_weight)\n",
    "\n",
    "        asset_added = False\n",
    "\n",
    "        for asset in ranked_assets.index:\n",
    "            if asset in new_portfolio:\n",
    "                continue\n",
    "\n",
    "            test_portfolio = new_portfolio + [asset]\n",
    "            portfolios_tested += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            new_returns, new_var, new_weighted_sharpe, new_p_return, new_p_variance, new_p_weights = _get_portfolio_stats(test_portfolio, risk_free_rate)\n",
    "            _update_portfolios_array(all_portfolios, test_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
    "\n",
    "            if new_weighted_sharpe > highest_weighted_sharpe:\n",
    "                best_iteration = portfolios_tested\n",
    "                best_portfolio = test_portfolio\n",
    "                curr_ret, curr_var = new_returns, new_var\n",
    "                highest_weighted_sharpe = new_weighted_sharpe\n",
    "\n",
    "                _update_portfolios_array(good_portfolios, test_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
    "\n",
    "                asset_added = True\n",
    "                break  # Accept first asset that improves Sharpe\n",
    "\n",
    "        if not asset_added:\n",
    "            print(\"All assets have been tested or no improvement found.\")\n",
    "            break\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    base_details = good_portfolios[0]\n",
    "    best_details = good_portfolios[-1]\n",
    "\n",
    "    return base_details, best_details, good_portfolios, all_portfolios, best_iteration  \n",
    "\n",
    "base_portfolio, best_portfolio, good_portfolios, total_portfolios, best_iteration = MLRBA_V1(names, cov, annualized_returns)\n",
    "base_portfolio, best_portfolio, len(good_portfolios), len(total_portfolios), best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Znh8vrYQabDW",
    "outputId": "f770bd56-eace-405c-a8c0-a1d835201f64"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[p[\"variance\"]**0.5 for p in good_portfolios],  # Convert variance to volatility\n",
    "    y=[p[\"return\"] for p in good_portfolios],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=[p[\"sharpe\"] for p in good_portfolios],  # Sharpe Ratio\n",
    "        showscale=True,\n",
    "        size=7,\n",
    "        line=dict(width=1),\n",
    "        colorscale=\"RdBu\",\n",
    "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[\n",
    "        f\"Return: {p['return']:.3%}<br>Volatility: {p['variance']**0.5:.3f}<br>\" +\n",
    "        f\"Sharpe Ratio: {p['return'] / (p['variance']**0.5):.3f}<br>\" +\n",
    "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
    "        for p in good_portfolios\n",
    "    ],\n",
    "    name=\"Portfolios\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Convergence Strategy Generarted Portfolios',\n",
    "    xaxis_title='Volatility (Standard Deviation)',\n",
    "    yaxis_title='Annualized Return',\n",
    "    legend=dict(y=5),\n",
    "    width=1920,\n",
    "    height=1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"ConvergenceRBA.html\")\n",
    "#fig.write_image(\"ConvergenceRBA.png\", format='png', width=1920, height=1080)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=volatility,\n",
    "    y=annualized_returns,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    hovertext=[\n",
    "        f\"{name} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
    "        for name, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
    "    ],\n",
    "    marker=dict(\n",
    "        color='green',\n",
    "        size=5,\n",
    "        line=dict(width=1)\n",
    "    ),\n",
    "    name=\"Individual Assets\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Convergence Strategy Generarted Portfolios with Individual Assets',\n",
    "    xaxis_title='Volatility (Standard Deviation)',\n",
    "    yaxis_title='Annualized Return',\n",
    "    legend=dict(y=5),\n",
    "    width=1920,\n",
    "    height=1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#fig.write_html(\"ConvergenceRBA+Asset.html\")\n",
    "#fig.write_image(\"ConvergenceRBA+Asset.png\", format='png', width=1920, height=1080)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fcrLEvL3abDW",
    "outputId": "7123cd39-a752-4640-be20-6fe3fcbc6c17"
   },
   "outputs": [],
   "source": [
    "sharpe_ratios = [portfolio['sharpe'] for portfolio in total_portfolios]\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=list(range(len(sharpe_ratios))), y=sharpe_ratios, mode='lines+markers'))\n",
    "fig.update_layout(title='Sharpe Ratio Over Iterations',\n",
    "                xaxis_title='Iteration',\n",
    "                yaxis_title='Sharpe Ratio',\n",
    "                width=1920,\n",
    "                height=1080,\n",
    "                font=dict(\n",
    "                    family=\"Cambria\",\n",
    "                    size=18,\n",
    "                )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing MLRBA_V1 with Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figC = go.Figure(fig2)\n",
    "figC.add_trace(go.Scatter(\n",
    "    x=[p[\"variance\"]**0.5 for p in good_portfolios],  # Convert variance to volatility\n",
    "    y=[p[\"return\"] for p in good_portfolios],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=7,\n",
    "        line=dict(width=1),\n",
    "        color=\"Red\",\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[\n",
    "        f\"Return: {p['return']:.3%}<br>Volatility: {p['variance']**0.5:.3f}<br>\" +\n",
    "        f\"Sharpe Ratio: {p['return'] / (p['variance']**0.5):.3f}<br>\" +\n",
    "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
    "        for p in good_portfolios\n",
    "    ],\n",
    "    name=\"Convergence Portfolios\"\n",
    "))\n",
    "\n",
    "figC.update_layout(\n",
    "    title='Convergence vs Monte Carlo vs Individual Assets',\n",
    "    legend=dict(x=0.85, y=0.95),\n",
    "    width=1920,\n",
    "    height=1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "figC.show()\n",
    "#figC.write_html(\"ComparisonOfConvergence+MC.html\")\n",
    "#figC.write_image(\"ComparisonOfConvergence+MC.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Reinforcement Weight Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLRBA_V2(ticker, covariances, returns, num_iterations=None, risk_free_rate = 0, \n",
    "             return_power = 1, std_power = 1, return_weight=1/3, corr_weight=1/3, vol_weight= 1/3, num_assets = 8, base_portfolio = None):\n",
    "    \n",
    "    if num_iterations is None:\n",
    "        num_iterations = min(math.comb(len(ticker), num_assets), 100000)\n",
    "\n",
    "    if base_portfolio is None:\n",
    "        base_portfolio = np.random.choice(list(ticker), num_assets, replace=False)\n",
    "        #base_portfolio = list(ticker)[:num_assets]\n",
    "\n",
    "    def _get_portfolio_stats(portfolio_assets, risk_free_rate=0):\n",
    "        p_asset_ret = returns.loc[portfolio_assets].values\n",
    "        p_asset_var = covariances.loc[portfolio_assets, portfolio_assets].values\n",
    "        best_p_weights = maximize_sharpe(p_asset_ret, p_asset_var)\n",
    "        p_ret = np.dot(best_p_weights, p_asset_ret)\n",
    "        p_var = np.dot(best_p_weights, p_asset_var @ best_p_weights)\n",
    "        sharpe = get_sharpe_ratio(p_ret, p_var, risk_free_rate, return_power, std_power)\n",
    "        return p_asset_ret, p_asset_var, sharpe, p_ret, p_var, best_p_weights\n",
    "\n",
    "    def _update_portfolios_array(portfolios, assets, weights, p_ret, p_var):\n",
    "        portfolios.append({\n",
    "            \"tickers\": assets,\n",
    "            \"weights\": weights,\n",
    "            \"return\": p_ret,\n",
    "            \"variance\": p_var,\n",
    "            \"sharpe\": (p_ret - risk_free_rate) / np.sqrt(p_var),\n",
    "        })\n",
    "\n",
    "    all_portfolios = []\n",
    "\n",
    "    curr_ret, curr_var, curr_weighted_sharpe, curr_p_return, curr_p_variance, curr_p_weights = _get_portfolio_stats(base_portfolio, risk_free_rate)\n",
    "    _update_portfolios_array(all_portfolios, base_portfolio, curr_p_weights, curr_p_return, curr_p_variance)\n",
    "\n",
    "    good_portfolios = all_portfolios.copy()\n",
    "    best_portfolio = base_portfolio.copy()\n",
    "\n",
    "    highest_weighted_sharpe = -np.inf\n",
    "    highest_weighted_sharpe = curr_weighted_sharpe\n",
    "\n",
    "    best_iteration = 0\n",
    "    portfolios_tested = 0\n",
    "\n",
    "    learning_rate = 0.03\n",
    "    improvement_threshold = 0.001\n",
    "\n",
    "    progress_bar = tqdm(total=num_iterations, desc=\"Portfolios Tested\")\n",
    "    for i in range(num_iterations):\n",
    "        asset_to_remove = find_best_asset_to_remove(best_portfolio, curr_var, curr_ret)\n",
    "        new_portfolio = [str(asset) for asset in best_portfolio if asset != asset_to_remove]\n",
    "\n",
    "        ranked_assets = find_asset_to_add(new_portfolio, ticker, covariances, returns, return_weight, corr_weight, vol_weight)\n",
    "\n",
    "        asset_added = False\n",
    "\n",
    "        for asset in ranked_assets.index:\n",
    "            portfolios_tested += 1\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            copy_new_portfolio = new_portfolio.copy()\n",
    "            copy_new_portfolio.append(asset)\n",
    "\n",
    "            new_returns, new_var, new_weighted_sharpe, new_p_return, new_p_variance, new_p_weights = _get_portfolio_stats(copy_new_portfolio, risk_free_rate)\n",
    "            _update_portfolios_array(all_portfolios, copy_new_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
    "\n",
    "            if new_weighted_sharpe > highest_weighted_sharpe:\n",
    "                best_iteration = portfolios_tested\n",
    "                improvement = new_weighted_sharpe - highest_weighted_sharpe\n",
    "                highest_weighted_sharpe = new_weighted_sharpe\n",
    "                best_portfolio = copy_new_portfolio\n",
    "                curr_ret, curr_var = new_returns, new_var\n",
    "\n",
    "                asset_added = True\n",
    "\n",
    "                asset_return = returns.loc[asset]\n",
    "                asset_vol = np.sqrt(covariances.loc[asset, asset])\n",
    "                avg_return = returns.mean()\n",
    "                avg_vol = np.sqrt(np.diag(covariances)).mean()\n",
    "\n",
    "                corr_with_portfolio = correlation_matrix.loc[copy_new_portfolio, asset].drop(asset).mean()\n",
    "                avg_corr_in_portfolio = correlation_matrix.loc[copy_new_portfolio].drop(asset, axis=1).mean().mean()\n",
    "\n",
    "                # Update weights using the current learning rate\n",
    "                return_weight += learning_rate * (asset_return - avg_return) / avg_return\n",
    "                vol_weight    += learning_rate * (avg_vol - asset_vol) / avg_vol\n",
    "                corr_weight   += learning_rate * (avg_corr_in_portfolio - corr_with_portfolio) / avg_corr_in_portfolio\n",
    "\n",
    "                total = return_weight + corr_weight + vol_weight\n",
    "                return_weight /= total\n",
    "                corr_weight /= total\n",
    "                vol_weight /= total\n",
    "\n",
    "                if improvement < improvement_threshold:\n",
    "                    learning_rate *= 0.95\n",
    "                else:\n",
    "                    learning_rate *= 1.01\n",
    "\n",
    "                _update_portfolios_array(good_portfolios, copy_new_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
    "                break  # stop at first valid improving asset\n",
    "\n",
    "        if not asset_added:\n",
    "            print(\"All assets have been tested or no improvement possible.\")\n",
    "            break\n",
    "\n",
    "    base_details = good_portfolios[0]\n",
    "    best_details = good_portfolios[-1]\n",
    "\n",
    "    return base_details, best_details, good_portfolios, all_portfolios, best_iteration\n",
    "\n",
    "\n",
    "base_portfolio, best_portfolio, good_portfolios, total_portfolios, best_iteration = MLRBA_V2(names, cov, annualized_returns)\n",
    "best_portfolio, best_portfolio['sharpe'], best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[p[\"variance\"]**0.5 for p in good_portfolios],  # Convert variance to volatility\n",
    "    y=[p[\"return\"] for p in good_portfolios],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=[p[\"sharpe\"] for p in good_portfolios],  # Sharpe Ratio\n",
    "        showscale=True,\n",
    "        size=7,\n",
    "        line=dict(width=1),\n",
    "        colorscale=\"RdBu\",\n",
    "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[\n",
    "        f\"Return: {p['return']:.3%}<br>Volatility: {p['variance']**0.5:.3f}<br>\" +\n",
    "        f\"Sharpe Ratio: {p['return'] / (p['variance']**0.5):.3f}<br>\" +\n",
    "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
    "        for p in good_portfolios\n",
    "    ],\n",
    "    name=\"Portfolios\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Learning Convergence Portfolios',\n",
    "    xaxis_title='Volatility (Standard Deviation)',\n",
    "    yaxis_title='Annualized Return',\n",
    "    legend=dict(y=5),\n",
    "    width=1920,\n",
    "    height=1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"LearningConvergenceRBA.html\")\n",
    "#fig.write_image(\"LearningConvergenceRBA.png\", format='png', width=1920, height=1080)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=volatility,\n",
    "    y=annualized_returns,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    hovertext=[\n",
    "        f\"{name} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
    "        for name, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
    "    ],\n",
    "    marker=dict(\n",
    "        color='green',\n",
    "        size=5,\n",
    "        line=dict(width=1)\n",
    "    ),\n",
    "    name=\"Individual Assets\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Learning Convergence Portfolios with Individual Assets',\n",
    "    xaxis_title='Volatility (Standard Deviation)',\n",
    "    yaxis_title='Annualized Return',\n",
    "    legend=dict(y=5),\n",
    "    width=1920,\n",
    "    height=1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"LearningConvergenceRBA+Asset.html\")\n",
    "#fig.write_image(\"LearningConvergenceRBA+Asset.png\", format='png', width=1920, height=1080)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratios = [portfolio['sharpe'] for portfolio in total_portfolios]\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=list(range(len(sharpe_ratios))), y=sharpe_ratios, mode='lines+markers'))\n",
    "fig.update_layout(title='Sharpe Ratio Over Iterations',\n",
    "                  xaxis_title='Iteration',\n",
    "                  yaxis_title='Sharpe Ratio',\n",
    "                  )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figC = go.Figure(fig2)\n",
    "figC.add_trace(go.Scatter(\n",
    "    x=[p[\"variance\"]**0.5 for p in good_portfolios],  # Convert variance to volatility\n",
    "    y=[p[\"return\"] for p in good_portfolios],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=7,\n",
    "        line=dict(width=1),\n",
    "        color=\"Red\",\n",
    "    ),\n",
    "    hoverinfo='text',\n",
    "    text=[\n",
    "        f\"Return: {p['return']:.3%}<br>Volatility: {p['variance']**0.5:.3f}<br>\" +\n",
    "        f\"Sharpe Ratio: {p['return'] / (p['variance']**0.5):.3f}<br>\" +\n",
    "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
    "        for p in good_portfolios\n",
    "    ],\n",
    "    name=\"Learning Convergence Portfolios\"\n",
    "))\n",
    "\n",
    "figC.update_layout(\n",
    "    title='Learning Convergence vs Monte Carlo vs Individual Assets',\n",
    "    legend=dict(x=0.8, y=0.95),\n",
    "    width=1920,\n",
    "    height=1080,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "figC.show()\n",
    "#figC.write_html(\"ComparisonOfLearningConvergence+MC.html\")\n",
    "#figC.write_image(\"ComparisonOfLearningConvergence+MC.png\", format='png', width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing V1 and V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MLRBA_multiple_times(names, cov, annualized_returns, num_runs, num_assets=8):\n",
    "\n",
    "    def generate_rand_port(tickers, num_assets, num_runs):\n",
    "        rand_port = []\n",
    "        for _ in range (num_runs):\n",
    "            base_portfolio = np.random.choice(list(tickers), num_assets, replace=False)\n",
    "            rand_port.append(base_portfolio)\n",
    "\n",
    "        return rand_port\n",
    "    \n",
    "    random_portfolios = generate_rand_port(names, num_assets=num_assets, num_runs=num_runs)\n",
    "\n",
    "    total_good_portfolios_length_v1 = 0\n",
    "    best_portfolios_v1 = []\n",
    "    best_iterations_v1 = []\n",
    "    \n",
    "    total_good_portfolios_length_v2 = 0\n",
    "    best_portfolios_v2 = []\n",
    "    best_iterations_v2 = []\n",
    "    \n",
    "    for portfolio in random_portfolios:\n",
    "        base_portfolio_v1, best_portfolio_v1, good_portfolios_v1, _, best_iteration_v1 = MLRBA_V1(names, cov, annualized_returns, base_portfolio=portfolio)\n",
    "        total_good_portfolios_length_v1 += len(good_portfolios_v1)\n",
    "        best_portfolios_v1.append(best_portfolio_v1)\n",
    "        best_iterations_v1.append(best_iteration_v1)\n",
    "        \n",
    "        base_portfolio_v2, best_portfolio_v2, good_portfolios_v2, _, best_iteration_v2 = MLRBA_V2(names, cov, annualized_returns, base_portfolio=portfolio)\n",
    "        total_good_portfolios_length_v2 += len(good_portfolios_v2)\n",
    "        best_portfolios_v2.append(best_portfolio_v2)\n",
    "        best_iterations_v2.append(best_iteration_v2)\n",
    "\n",
    "        print(base_portfolio_v1['tickers'] == base_portfolio_v2['tickers'])\n",
    "\n",
    "    average_length_v1 = total_good_portfolios_length_v1 / num_runs\n",
    "    average_iteration_v1 = statistics.fmean(best_iterations_v1)\n",
    "    std_dev_iteration_v1 = statistics.stdev(best_iterations_v1) if num_runs > 1 else 0\n",
    "\n",
    "    average_length_v2 = total_good_portfolios_length_v2 / num_runs\n",
    "    average_iteration_v2 = statistics.fmean(best_iterations_v2)\n",
    "    std_dev_iteration_v2 = statistics.stdev(best_iterations_v2) if num_runs > 1 else 0\n",
    "\n",
    "    \n",
    "    results = {\n",
    "        'v1': (base_portfolio_v1, average_length_v1, best_portfolios_v1, average_iteration_v1, std_dev_iteration_v1, best_iterations_v1),\n",
    "        'v2': (base_portfolio_v2, average_length_v2, best_portfolios_v2, average_iteration_v2, std_dev_iteration_v2, best_iterations_v2)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "num_runs = 20\n",
    "results = run_MLRBA_multiple_times(names, cov, annualized_returns, num_runs)\n",
    "\n",
    "_, _, best_portfolios_v1, average_iteration_v1, std_dev_v1, best_iterations_v1 = results['v1']\n",
    "_, _, best_portfolios_v2, average_iteration_v2, std_dev_v2, best_iterations_v2 = results['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "avg_sharpe_v1 = np.mean([portfolio['sharpe'] for portfolio in best_portfolios_v1])\n",
    "avg_sharpe_v2 = np.mean([portfolio['sharpe'] for portfolio in best_portfolios_v2])\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Average Iterations to Find Best Portfolio\",\n",
    "        \"Average Highest Sharpe Ratio\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=['Standard Convergence', 'Learning Convergergence'],\n",
    "    y=[average_iteration_v1, average_iteration_v2],\n",
    "    name='Iterations',\n",
    "    error_y=dict(type='data', array=[std_dev_v1, std_dev_v2], visible=True),\n",
    "    width=0.4\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=['Standard Convergence', 'Learning Convergergence'],\n",
    "    y=[avg_sharpe_v1, avg_sharpe_v2],\n",
    "    name='Sharpe Ratio',\n",
    "    width=0.4\n",
    "), row=1, col=2)\n",
    "\n",
    "# Axis titles\n",
    "fig.update_xaxes(title_text='Method Version', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Average Iterations', row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text='Method Version', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Average Sharpe Ratio', row=1, col=2)\n",
    "\n",
    "# Layout and display\n",
    "fig.update_layout(\n",
    "    title_text='Standard Convergence vs Learning Convergence',\n",
    "    showlegend=False,\n",
    "    font=dict(\n",
    "        family=\"Cambria\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"StandardvsLearning.html\")\n",
    "#fig.write_image(\"StandardvsLearning.png\", format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Portfolio Prediction using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioPredictor:\n",
    "    def __init__(self, raw_data_train, raw_data_test, best_portfolio, n_steps=1, epochs=50, batch_size=32):\n",
    "        self.raw_data_train = raw_data_train\n",
    "        self.raw_data_test = raw_data_test\n",
    "        self.best_portfolio = best_portfolio\n",
    "        self.n_steps = n_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        best_portfolio_data_train = self.raw_data_train[self.best_portfolio['tickers']]\n",
    "        best_portfolio_data_test = self.raw_data_test[self.best_portfolio['tickers']]\n",
    "        weights = np.array(self.best_portfolio['weights'])\n",
    "\n",
    "        # Use a scaler fitted on a broader dataset so that training/test normalization is consistent\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # Fit on the entire raw_data (or on a fixed training period) for consistency\n",
    "        full_data = pd.concat([best_portfolio_data_train, best_portfolio_data_test])\n",
    "        self.scaler.fit(full_data)\n",
    "        \n",
    "        normalized_train_data = self.scaler.transform(best_portfolio_data_train)\n",
    "        normalized_test_data = self.scaler.transform(best_portfolio_data_test)\n",
    "\n",
    "        self.weighted_returns_train = np.dot(normalized_train_data, weights)\n",
    "        self.weighted_returns_test = np.dot(normalized_test_data, weights)\n",
    "\n",
    "    def create_datasets(self, data):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.n_steps):\n",
    "            v = data[i:(i + self.n_steps), :]\n",
    "            X.append(v)\n",
    "            y.append(data[i + self.n_steps, :])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Sequential([\n",
    "            LSTM(250, activation='relu', return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, activation='relu', return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ])\n",
    "\n",
    "        def tf_weighted_mse(y_true, y_pred, power=3):\n",
    "            n = tf.shape(y_true)[0]\n",
    "            normalized_index = tf.cond(\n",
    "                tf.equal(n, 1),\n",
    "                lambda: tf.ones([n], dtype=tf.float32),\n",
    "                lambda: tf.cast(tf.range(n), tf.float32) / tf.cast(n - 1, tf.float32)\n",
    "            )\n",
    "            weights = tf.pow(normalized_index, power)\n",
    "            weights += 1e-6\n",
    "            weights /= tf.reduce_sum(weights)\n",
    "            \n",
    "            squared_errors = tf.square(y_true - y_pred)\n",
    "            weighted_squared_errors = weights * squared_errors\n",
    "            return tf.reduce_mean(weighted_squared_errors)\n",
    "\n",
    "        self.model.compile(optimizer='adam', loss=tf_weighted_mse)\n",
    "\n",
    "    def train_model(self):\n",
    "        self.X_train_weighted, self.y_train_weighted = self.create_datasets(self.weighted_returns_train.reshape(-1, 1))\n",
    "        self.history = self.model.fit(self.X_train_weighted, self.y_train_weighted, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.001, shuffle=False, verbose=0)\n",
    "\n",
    "    def predict(self):\n",
    "        X_test_weighted, y_test_weighted = self.create_datasets(self.weighted_returns_test.reshape(-1, 1))\n",
    "        \n",
    "        self.predictions = self.model.predict(X_test_weighted)\n",
    "        self.y_test_weighted = y_test_weighted\n",
    "        \n",
    "        return self.predictions\n",
    "\n",
    "    def normalize_cumulative_returns(self, data):\n",
    "        data_series = pd.Series(data.flatten())\n",
    "        pct_change = data_series.pct_change().fillna(0)\n",
    "        cum_returns = (1 + pct_change).cumprod()\n",
    "        normalized_returns = cum_returns * 100\n",
    "        return normalized_returns\n",
    "\n",
    "    def normalize_cumulative_returns_with_baseline(self, data, baseline):\n",
    "        data_series = pd.Series(data.flatten())\n",
    "        pct_change = data_series.pct_change().fillna(0)\n",
    "        cum_returns = (1 + pct_change).cumprod()\n",
    "        normalized_returns = cum_returns * baseline\n",
    "        return normalized_returns\n",
    "\n",
    "    def plot_loss(self):\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=np.arange(1, len(self.history.history['loss'])+1), y=self.history.history['loss'], mode='lines', name='Training Loss'))\n",
    "        fig.add_trace(go.Scatter(x=np.arange(1, len(self.history.history['val_loss'])+1), y=self.history.history['val_loss'], mode='lines', name='Validation Loss'))\n",
    "        fig.update_layout(title='Training and Validation Loss Over Epochs',\n",
    "                          xaxis_title='Epoch',\n",
    "                          yaxis_title='Loss',\n",
    "                          legend_title='Type of Loss')\n",
    "        fig.show()\n",
    "        \n",
    "    def plot_predictions(self):\n",
    "        normalized_train = self.normalize_cumulative_returns(self.y_train_weighted)\n",
    "        training_end_value = normalized_train.iloc[-1]\n",
    "        normalized_test = self.normalize_cumulative_returns_with_baseline(self.y_test_weighted, training_end_value)\n",
    "        normalized_predicted = self.normalize_cumulative_returns_with_baseline(self.predictions, training_end_value)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.raw_data_train.index,\n",
    "            y=normalized_train,\n",
    "            mode='lines',\n",
    "            name='Actual Training Returns'\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.raw_data_test.index[self.n_steps:],\n",
    "            y=normalized_test,\n",
    "            mode='lines',\n",
    "            name='Actual Test Returns'\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.raw_data_test.index[self.n_steps:],\n",
    "            y=normalized_predicted,\n",
    "            mode='lines',\n",
    "            name='Predicted Test Returns'\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title='Actual vs Predicted Weighted Portfolio Returns',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Normalized Returns',\n",
    "            legend_title='Portfolio'\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_length = 500\n",
    "investment_length -= 1\n",
    "new_end_date = add_days_to_date(end_date, investment_length)\n",
    "\n",
    "raw_data, asset_errors, max_combination= fetch_raw_data_yf(assets, start_date, new_end_date)\n",
    "names, annualized_returns, unweighted_annaulized_returns, weighted_returns_matrix, normal_returns_matrix, cov, correlation_matrix = get_matrices(raw_data)\n",
    "\n",
    "split = len(raw_data.index) - investment_length\n",
    "raw_data_train = raw_data.iloc[:split]\n",
    "raw_data_test = raw_data.iloc[split:]\n",
    "\n",
    "portfolio_predictor = PortfolioPredictor(raw_data_train, raw_data_test, best_portfolio, n_steps=3, epochs=30)\n",
    "\n",
    "portfolio_predictor.preprocess_data()\n",
    "portfolio_predictor.build_model()\n",
    "portfolio_predictor.train_model()\n",
    "prediction = portfolio_predictor.predict()   \n",
    "portfolio_predictor.plot_loss()\n",
    "portfolio_predictor.plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_portfolios_over_time(raw_data, window_size=5, num_windows=None, threshold=0.05, epochs=30):\n",
    "    split = len(raw_data.index) - investment_length\n",
    "    all_good_portfolios = []\n",
    "    if num_windows is None:\n",
    "        num_windows = investment_length // window_size\n",
    "    \n",
    "    previous_best_portfolio = None \n",
    "\n",
    "    for i in tqdm(range(num_windows)):\n",
    "        curr_split = i * window_size\n",
    "\n",
    "        loop_raw_data_train = raw_data.iloc[:split + curr_split]\n",
    "        loop_raw_data_test = raw_data.iloc[split + curr_split:]\n",
    "        loop_names, loop_annualized_returns, _, _, _, loop_cov, _ = get_matrices(loop_raw_data_train)\n",
    "        \n",
    "        _, loop_best_portfolio, loop_good_portfolios, _, _ = MLRBA_V2(loop_names, loop_cov, loop_annualized_returns)\n",
    "        best_sharpe = loop_best_portfolio['sharpe']\n",
    "        \n",
    "        close_to_best = []\n",
    "        if previous_best_portfolio is not None:\n",
    "            close_to_best.append(previous_best_portfolio)\n",
    "        close_to_best.append(loop_best_portfolio)\n",
    "        \n",
    "        for j in range(len(loop_best_portfolio)):\n",
    "            difference = abs((best_sharpe - loop_good_portfolios[j]['sharpe']) / best_sharpe)\n",
    "            if difference < threshold:\n",
    "                close_to_best.append(loop_good_portfolios[j])\n",
    "\n",
    "        print(f'Length of close to best is: {len(close_to_best)}')\n",
    "\n",
    "        sharpe_list = [portfolio['sharpe'] for portfolio in close_to_best]\n",
    "        print(\"Sharpe ratios (first is best_sharpe):\", sharpe_list)\n",
    "        \n",
    "        portfolio_results = {}\n",
    "        for id, portfolio in enumerate(close_to_best):\n",
    "            portfolio_predictor = PortfolioPredictor(loop_raw_data_train, loop_raw_data_test, portfolio, n_steps=window_size, epochs=epochs)\n",
    "            portfolio_predictor.preprocess_data()\n",
    "            portfolio_predictor.build_model()\n",
    "            portfolio_predictor.train_model()\n",
    "            prediction = portfolio_predictor.predict() \n",
    "\n",
    "            if len(prediction) >= window_size:\n",
    "                end_pred = prediction[window_size-1]\n",
    "            else:\n",
    "                end_pred = prediction[-1]\n",
    "            \n",
    "            percentage_diff = (end_pred - prediction[0]) / prediction[0]\n",
    "            print(prediction[:min(window_size, len(prediction))], prediction[0], percentage_diff * 100)          \n",
    "            \n",
    "            portfolio_results[id] = percentage_diff\n",
    "\n",
    "        best_id = None\n",
    "\n",
    "        # Check if all predictions (percentage_diff) are negative\n",
    "        if max(portfolio_results.values()) < 0:\n",
    "            print(\"All percentage differences are negative. Choosing an empty portfolio (not holding anything).\")\n",
    "            predicted_best_portfolio = {}\n",
    "        else:\n",
    "            best_id = max(portfolio_results, key=portfolio_results.get)\n",
    "            predicted_best_portfolio = close_to_best[best_id]\n",
    "            previous_best_portfolio = predicted_best_portfolio\n",
    "        \n",
    "        start_date = loop_raw_data_test.index[0]\n",
    "        end_date = loop_raw_data_test.index[window_size-1]\n",
    "        \n",
    "        all_good_portfolios.append({\n",
    "            \"portfolio\": predicted_best_portfolio,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date\n",
    "        })\n",
    "        if best_id is not None:\n",
    "            print(f'Current iteration: {i}, the best portfolio found was portfolio: {best_id}')\n",
    "        else:\n",
    "            print(f'Current iteration: {i}, no portfolio selected (empty portfolio chosen).')\n",
    "    \n",
    "    return all_good_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_good_portfolios = evaluate_portfolios_over_time(raw_data, window_size=5, num_windows=None, threshold=0.5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_asset_returns(raw_data, assets, start_date, end_date):\n",
    "    if not isinstance(raw_data.index, pd.DatetimeIndex):\n",
    "        raw_data.index = pd.to_datetime(raw_data.index)\n",
    "\n",
    "    filtered_data = raw_data.loc[start_date:end_date, assets]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def chain_portfolio_performance(weekly_series_list, starting_value=100):\n",
    "    continuous_series = pd.Series()\n",
    "    current_value = starting_value\n",
    "\n",
    "    for week_series in weekly_series_list:\n",
    "        # Normalize the week so that it starts at 1 (or current_value)\n",
    "        week_normalized = week_series / week_series.iloc[0]\n",
    "        # Scale the normalized week to start at current_value\n",
    "        week_scaled = week_normalized * current_value\n",
    "        # Update the current_value to the last value of this week\n",
    "        current_value = week_scaled.iloc[-1]\n",
    "        # Append the week_series to the continuous_series\n",
    "        continuous_series = pd.concat([continuous_series, week_scaled])\n",
    "    \n",
    "    return continuous_series\n",
    "\n",
    "ML_portfolio = []\n",
    "initial_value = 100\n",
    "\n",
    "for i in range(len(all_good_portfolios)):\n",
    "    curr_best_portfolio = all_good_portfolios[i]['portfolio']\n",
    "    p_start_date = all_good_portfolios[i]['start_date']\n",
    "    p_end_date = all_good_portfolios[i]['end_date']\n",
    "    \n",
    "    if not curr_best_portfolio:\n",
    "        if i == 0:\n",
    "            previous_value = initial_value\n",
    "        else:\n",
    "            previous_value = ML_portfolio[-1].iloc[-1]\n",
    "        # Create a series with the same index as in raw_data for the window duration, all at previous_value.\n",
    "        window_index = raw_data.loc[p_start_date:p_end_date].index\n",
    "        portfolio_daily_returns = pd.Series(previous_value, index=window_index)\n",
    "    else:\n",
    "        best_curr_port_assets = curr_best_portfolio['tickers']\n",
    "        best_curr_port_assets_test_data = extract_asset_returns(raw_data, best_curr_port_assets, p_start_date, p_end_date)\n",
    "        curr_best_portfolio_weights = curr_best_portfolio['weights']\n",
    "        weighted_returns = best_curr_port_assets_test_data.mul(curr_best_portfolio_weights, axis='columns')\n",
    "        portfolio_daily_returns = weighted_returns.sum(axis=1)\n",
    "    \n",
    "    ML_portfolio.append(portfolio_daily_returns)\n",
    "\n",
    "ML_portfolio_streamed = chain_portfolio_performance(ML_portfolio, starting_value=initial_value)\n",
    "ML_portfolio_streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_daily_returns = ML_portfolio_streamed.pct_change()\n",
    "ML_cumulative_returns = (1 + ML_daily_returns).cumprod()\n",
    "\n",
    "ML_cumulative_returns.iloc[0] = 1\n",
    "ML_portfolio_normalized = (ML_cumulative_returns / ML_cumulative_returns.iloc[0]) * 100\n",
    "\n",
    "Nasdaq_comp = getNasdaq_comp(ML_portfolio_streamed.index[0], ML_portfolio_streamed.index[-1])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Nasdaq_comp.index,\n",
    "    y=Nasdaq_comp['Normalized'],\n",
    "    mode='lines',\n",
    "    name='Nasdaq Composite'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ML_cumulative_returns.index,\n",
    "    y=ML_portfolio_normalized,\n",
    "    mode='lines',\n",
    "    name='Portfolio Growth'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparison of Portfolio vs. Nasdaq Composite Growth',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Normalized Value (Base 100%)',\n",
    "    xaxis=dict(\n",
    "        type='date',\n",
    "        tickformat='%b %Y',\n",
    "        tickmode='auto'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5u_KAQS1CRr"
   },
   "source": [
    "## 7.0 Testing Against Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "n4AB0laC1CRr",
    "outputId": "4b139451-2662-435b-dc3d-56a5eed5b189"
   },
   "outputs": [],
   "source": [
    "best_port_assets = best_portfolio['tickers']\n",
    "best_port_assets_test_data = raw_data_test.loc[:, best_port_assets]\n",
    "\n",
    "Nasdaq_comp = getNasdaq_comp(ML_cumulative_returns.index[0], ML_cumulative_returns.index[-1])\n",
    "\n",
    "best_portfolio_weights = best_portfolio['weights']\n",
    "normalized_prices = best_port_assets_test_data.div(best_port_assets_test_data.iloc[0])\n",
    "daily_returns = normalized_prices.pct_change()\n",
    "weighted_returns = daily_returns.mul(best_portfolio_weights, axis='columns')\n",
    "portfolio_daily_returns = weighted_returns.sum(axis=1)\n",
    "portfolio_cumulative_returns = (1 + portfolio_daily_returns).cumprod()\n",
    "\n",
    "portfolio_start = portfolio_cumulative_returns.iloc[0]\n",
    "portfolio_normalized = (portfolio_cumulative_returns / portfolio_start) * 100\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Nasdaq_comp.index,\n",
    "    y=Nasdaq_comp['Normalized'],\n",
    "    mode='lines',\n",
    "    name='Nasdaq Composite'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ML_cumulative_returns.index,\n",
    "    y=portfolio_normalized,\n",
    "    mode='lines',\n",
    "    name='Portfolio Growth'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ML_cumulative_returns.index,\n",
    "    y=ML_portfolio_normalized,\n",
    "    mode='lines',\n",
    "    name='Portfolio Growth'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparison of Portfolio vs. Nasdaq Composite Growth',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Normalized Value (Base 100)',\n",
    "    xaxis=dict(\n",
    "        type='date',\n",
    "        tickformat='%b %Y',\n",
    "        tickmode='auto'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Find Optimal Portfolio Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_portfolios, dominant_portfolios = MonteCarloRBA(names, cov, annualized_returns, 10000, 'sharpe', 3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_portfolio_sizes = [len(portfolio['tickers']) for portfolio in all_portfolios]\n",
    "rd_volatility = [np.sqrt(portfolio['variance']) for portfolio in all_portfolios]\n",
    "rd_returns = [portfolio['return'] for portfolio in all_portfolios]\n",
    "\n",
    "volatility_by_size = defaultdict(list)\n",
    "for size, vol, ret in zip(rd_portfolio_sizes, rd_volatility, rd_returns):\n",
    "    volatility_by_size[size].append((vol, ret))\n",
    "\n",
    "average_volatility = {size: np.mean([v[0] for v in vols]) for size, vols in volatility_by_size.items()}\n",
    "average_returns = {size: np.mean([v[1] for v in vols]) for size, vols in volatility_by_size.items()}\n",
    "\n",
    "sorted_sizes = sorted(average_volatility.keys())\n",
    "sorted_average_vols = [average_volatility[size] for size in sorted_sizes]\n",
    "sorted_average_rets = [average_returns[size] for size in sorted_sizes]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sorted_sizes,\n",
    "    y=sorted_average_vols,\n",
    "    mode='lines',\n",
    "    name='Average Volatility'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average Volatility and Returns by Portfolio Size',\n",
    "    xaxis_title='Number of Assets in Portfolio',\n",
    "    yaxis_title='Average Value',\n",
    "    xaxis=dict(type='category'),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_portfolios, dominant_portfolios = MonteCarloRBA(names, cov, annualized_returns, 10000, 'vol', 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_portfolio_sizes = [sum(weight > 0 for weight in portfolio['weights']) for portfolio in all_portfolios]\n",
    "\n",
    "portfolio_size_counts = Counter(rd_portfolio_sizes)\n",
    "\n",
    "sizes = sorted(portfolio_size_counts.keys())\n",
    "counts = [portfolio_size_counts[size] for size in sizes]\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=sizes, y=counts)])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Portfolio Sizes After Optimization',\n",
    "    xaxis_title='Number of assets in portfolio after optimizing',\n",
    "    yaxis_title='Number of Portfolios',\n",
    "    xaxis=dict(type='category'),\n",
    "    yaxis=dict(type='linear')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.write_html(\"PortfolioSize.html\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "roboA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
