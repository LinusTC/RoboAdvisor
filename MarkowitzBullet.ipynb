{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVQVoEjG1FQq",
        "outputId": "ca1ebd6b-484d-486e-dced-3736403f1e3f"
      },
      "outputs": [],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OxPgoZ41Q9G",
        "outputId": "a9cbe06e-4000-4d98-b7a2-6c2c9e1a3e4a"
      },
      "outputs": [],
      "source": [
        "'''cd drive/MyDrive/RoboA/'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj91ThpmabDR"
      },
      "source": [
        "# Markowitz Efficient Frontier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w31yow0TabDS"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvpNgOhEabDS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import statistics\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from itertools import combinations\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential # type: ignore\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout # type: ignore\n",
        "\n",
        "from fetchData import fetch_raw_data_yf, getSNP500, fetch_raw_data_yf_all, getNasdaq_comp\n",
        "from MonteCarloRBA import MonteCarloRBA\n",
        "from PortfolioFunction import maximize_sharpe, create_correlation_matrix, get_sharpe_ratio, get_matrices\n",
        "from LearningRBA import find_best_asset_to_remove, find_asset_to_add\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnySmmT6abDT"
      },
      "source": [
        "## 2. Fetch Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bz1yuGUabDT"
      },
      "source": [
        "### Get all Nasdaq Stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4G7yoJuabDT",
        "outputId": "e6e2de66-44f1-4d77-8bc1-bfe43fe6e4cd"
      },
      "outputs": [],
      "source": [
        "assets= [\n",
        "    \"AAPL\",  # Apple Inc.\n",
        "    \"MSFT\",  # Microsoft Corporation\n",
        "    \"AMZN\",  # Amazon.com Inc.\n",
        "    \"GOOGL\", # Alphabet Inc. (Google) Class A\n",
        "    \"GOOG\",  # Alphabet Inc. (Google) Class C\n",
        "    \"META\",    # Meta Platforms Inc (formerly Facebook)\n",
        "    \"TSLA\",  # Tesla Inc\n",
        "    \"UA\", # Berkshire Hathaway Inc. Class B\n",
        "    \"JPM\",   # JPMorgan Chase & Co.\n",
        "    \"V\",     # Visa Inc.\n",
        "    \"JNJ\",   # Johnson & Johnson\n",
        "    \"WMT\",   # Walmart Inc.\n",
        "    \"PG\",    # Procter & Gamble Co.\n",
        "    \"UNH\",   # UnitedHealth Group Inc.\n",
        "    \"MA\",    # Mastercard Inc.\n",
        "    \"NVDA\",  # NVIDIA Corporation\n",
        "    \"HD\",    # Home Depot Inc.\n",
        "    \"BAC\",   # Bank of America Corp\n",
        "    \"DIS\",   # Walt Disney Co\n",
        "    \"PYPL\",  # PayPal Holdings\n",
        "    \"VZ\",    # Verizon Communications Inc.\n",
        "    \"ADBE\",  # Adobe Inc.\n",
        "    \"CMCSA\", # Comcast Corporation\n",
        "    \"NFLX\",  # Netflix Inc.\n",
        "    \"KO\",    # Coca-Cola Co\n",
        "    \"NKE\",   # NIKE Inc.\n",
        "    \"PFE\",   # Pfizer Inc.\n",
        "    \"MRK\",   # Merck & Co., Inc.\n",
        "    \"PEP\",   # PepsiCo, Inc.\n",
        "    \"T\",     # AT&T Inc.\n",
        "    \"ABT\",   # Abbott Laboratories\n",
        "    \"CRM\",   # Salesforce.com Inc.\n",
        "    \"ORCL\",  # Oracle Corporation\n",
        "    \"ABBV\",  # AbbVie Inc.\n",
        "    \"CSCO\",  # Cisco Systems, Inc.\n",
        "    \"INTC\",  # Intel Corporation\n",
        "    \"TMO\",   # Thermo Fisher Scientific Inc.\n",
        "    \"XOM\",   # Exxon Mobil Corporation\n",
        "    \"ACN\",   # Accenture plc\n",
        "    \"LLY\",   # Eli Lilly and Company\n",
        "    \"COST\",  # Costco Wholesale Corporation\n",
        "    \"MCD\",   # McDonald's Corp\n",
        "    \"DHR\",   # Danaher Corporation\n",
        "    \"MDT\",   # Medtronic plc\n",
        "    \"NEE\",   # NextEra Energy, Inc.\n",
        "    \"BMY\",   # Bristol-Myers Squibb Company\n",
        "    \"QCOM\",  # Qualcomm Inc\n",
        "    \"CVX\",   # Chevron Corporation\n",
        "    \"WFC\",   # Wells Fargo & Co\n",
        "    \"LMT\",    # Lockheed Martin Corporation\n",
        "    \"GS\",   # Goldman Sachs Group, Inc.\n",
        "    \"MS\",   # Morgan Stanley\n",
        "    \"IBM\",  # International Business Machines Corporation\n",
        "    \"GE\",   # General Electric Company\n",
        "    \"F\",    # Ford Motor Company\n",
        "    \"GM\",   # General Motors Company\n",
        "    \"UBER\", # Uber Technologies, Inc.\n",
        "    \"LYFT\", # Lyft, Inc.\n",
        "    \"SNAP\", # Snap Inc.\n",
        "    \"TWTR\", # Twitter, Inc.\n",
        "    \"SPOT\", # Spotify Technology S.A.\n",
        "    \"AMD\",  # Advanced Micro Devices, Inc.\n",
        "    \"TXN\",  # Texas Instruments Incorporated\n",
        "    \"BABA\", # Alibaba Group Holding Limited\n",
        "    \"SAP\",  # SAP SE\n",
        "    \"HON\",  # Honeywell International Inc.\n",
        "    \"BA\",   # Boeing Company\n",
        "    \"RTX\",  # Raytheon Technologies Corporation\n",
        "    \"CAT\",  # Caterpillar Inc.\n",
        "    \"DE\",   # Deere & Company\n",
        "    \"MMM\",  # 3M Company\n",
        "    \"DUK\",  # Duke Energy Corporation\n",
        "    \"SO\",   # Southern Company\n",
        "    \"EXC\",  # Exelon Corporation\n",
        "    \"NEE\",  # NextEra Energy, Inc.\n",
        "    \"AEP\",  # American Electric Power Company, Inc.\n",
        "    \"SRE\",  # Sempra Energy\n",
        "    \"ETN\",  # Eaton Corporation plc\n",
        "    \"EMR\",  # Emerson Electric Co.\n",
        "    \"SYY\",  # Sysco Corporation\n",
        "    \"KR\",   # Kroger Co.\n",
        "    \"GIS\",  # General Mills, Inc.\n",
        "    \"K\",    # Kellogg Company\n",
        "    \"CPB\",  # Campbell Soup Company\n",
        "    \"MO\",   # Altria Group, Inc.\n",
        "    \"PM\",   # Philip Morris International Inc.\n",
        "    \"BTI\",  # British American Tobacco plc\n",
        "    \"RDY\",  # Dr. Reddy's Laboratories Ltd.\n",
        "    \"GILD\", # Gilead Sciences, Inc.\n",
        "    \"BIIB\", # Biogen Inc.\n",
        "    \"CELG\", # Celgene Corporation\n",
        "    \"AMGN\", # Amgen Inc.\n",
        "    \"SYK\",  # Stryker Corporation\n",
        "    \"BSX\",  # Boston Scientific Corporation\n",
        "    \"ISRG\", # Intuitive Surgical, Inc.\n",
        "    \"ZBH\",  # Zimmer Biomet Holdings, Inc.\n",
        "    \"EW\",   # Edwards Lifesciences Corporation\n",
        "    \"RMD\",  # ResMed Inc.\n",
        "    \"VRTX\", # Vertex Pharmaceuticals Incorporated\n",
        "    \"REGN\",  # Regeneron Pharmaceuticals, Inc.\n",
        "]\n",
        "\n",
        "#assets = getSNP500()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvJfF_5YabDU",
        "outputId": "e2769ab4-6379-4ccc-e628-557b92733cd1"
      },
      "outputs": [],
      "source": [
        "start_date = \"2015-01-01\"\n",
        "end_date = \"2018-01-01\"\n",
        "raw_data, asset_errors, max_combination= fetch_raw_data_yf_all(assets, start_date, end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtnI4d7k1CRp"
      },
      "source": [
        "### Split into test and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUQlY2uT1CRp"
      },
      "outputs": [],
      "source": [
        "split = len(raw_data.index) // 2\n",
        "\n",
        "raw_data_train = raw_data.iloc[:split]\n",
        "raw_data_test = raw_data.iloc[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyXEENSbabDU"
      },
      "source": [
        "## 3. Mean, Volatility and Covariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8UnmVllabDU",
        "outputId": "5db2e605-2732-40fd-8052-5cddc38fded9"
      },
      "outputs": [],
      "source": [
        "names, annualized_returns, cov, correlation_matrix = get_matrices(raw_data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volatility = np.sqrt(np.diag(cov))\n",
        "\n",
        "risk_free_rate=0\n",
        "sharpe_ratios = (annualized_returns - risk_free_rate) / volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "70g4vm3qabDU",
        "outputId": "3974e87c-1b85-4392-e8b8-594c2626466e"
      },
      "outputs": [],
      "source": [
        "hover_texts = [\n",
        "    f\"{ticker} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
        "    for ticker, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
        "]\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(\n",
        "    x=volatility,\n",
        "    y=annualized_returns,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    hovertext=hover_texts,\n",
        "    marker=dict(color=sharpe_ratios, colorscale = 'RdBu', size=6, line=dict(width=1), colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
        "    )\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Markowitz Mean Varience Model',\n",
        "    xaxis_title='Volatility (Standard Deviation)',\n",
        "    yaxis_title='Annualized Returns',\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3i7iFfDabDV"
      },
      "source": [
        "## 4.0 Monte Carlo Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsgRqCSWabDV",
        "outputId": "55346305-b21e-48aa-bd8b-0daed1a50060"
      },
      "outputs": [],
      "source": [
        "all_portfolios, dominant_portfolios = MonteCarloRBA(names, cov, annualized_returns, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09tYBpwxabDV",
        "outputId": "055bc292-d45d-4b5b-baaf-482d1c4ede81"
      },
      "outputs": [],
      "source": [
        "print (len(dominant_portfolios) ,len(all_portfolios))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "uuj_oTIZabDV",
        "outputId": "c0fec4f3-4492-4608-a54f-48d800b04ba7"
      },
      "outputs": [],
      "source": [
        "fig1 = go.Figure()\n",
        "\n",
        "fig1.add_trace(go.Scatter(\n",
        "    x=[np.sqrt(p[\"variance\"]) for p in all_portfolios],\n",
        "    y=[p[\"return\"] for p in all_portfolios],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color=[p[\"sharpe\"] for p in all_portfolios],\n",
        "        showscale=True,\n",
        "        size=7,\n",
        "        line=dict(width=1),\n",
        "        colorscale=\"RdBu\",\n",
        "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
        "    ),\n",
        "    hoverinfo='text',\n",
        "    text=[\n",
        "        f\"Return: {p['return']:.3%}<br>Volatility: {np.sqrt(p['variance']):.3f}<br>\" +\n",
        "        f\"Sharpe Ratio: {p['return'] / (np.sqrt(p['variance'])):.3f}<br>\" +\n",
        "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
        "        for p in all_portfolios\n",
        "    ]\n",
        "))\n",
        "\n",
        "fig1.update_layout(\n",
        "    xaxis=dict(title='Volatility (Standard Deviation)'),\n",
        "    yaxis=dict(title='Annualised Returns'),\n",
        "    title='Sample of Random Portfolios'\n",
        ")\n",
        "\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "uHkH6APvabDV",
        "outputId": "8dbda4b6-b409-48de-98ea-c89f8e6f5f85"
      },
      "outputs": [],
      "source": [
        "fig2 = go.Figure()\n",
        "\n",
        "fig2.add_trace(go.Scatter(\n",
        "    x=[np.sqrt(p[\"variance\"]) for p in dominant_portfolios],  # Convert variance to volatility\n",
        "    y=[p[\"return\"] for p in dominant_portfolios],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color=[p[\"return\"] / (np.sqrt(p[\"variance\"])) for p in dominant_portfolios],  # Sharpe Ratio\n",
        "        showscale=True,\n",
        "        size=7,\n",
        "        line=dict(width=1),\n",
        "        colorscale=\"RdBu\",\n",
        "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
        "    ),\n",
        "    hoverinfo='text',\n",
        "    text=[\n",
        "        f\"Return: {p['return']:.3%}<br>Volatility: {np.sqrt(p['variance']):.3f}<br>\" +\n",
        "        f\"Sharpe Ratio: {p['sharpe']:.3f}<br>\" +\n",
        "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
        "        for p in dominant_portfolios\n",
        "    ],\n",
        "    name=\"Portfolios\"\n",
        "))\n",
        "\n",
        "fig2.add_trace(go.Scatter(\n",
        "    x=volatility,\n",
        "    y=annualized_returns,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    hovertext=[\n",
        "        f\"{name} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
        "        for name, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
        "    ],\n",
        "    marker=dict(\n",
        "        color='brown',\n",
        "        size=5,\n",
        "        symbol='triangle-up',  # Sets the marker shape to a triangle\n",
        "        line=dict(width=1)\n",
        "    ),\n",
        "    name=\"Individual Assets\"\n",
        "))\n",
        "\n",
        "fig2.update_layout(\n",
        "    title='Sample of Random Portfolios',\n",
        "    xaxis_title='Volatility (Standard Deviation)',\n",
        "    yaxis_title='Annualized Return',\n",
        "    legend=dict(y=5\n",
        "    )\n",
        ")\n",
        "\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-WDtJTgabDV"
      },
      "source": [
        "## 5.0 Machine Learning Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Optimization Function Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xbdm-4NabDV",
        "outputId": "53c0d514-7544-4e8f-9ed5-4af9aa786a56"
      },
      "outputs": [],
      "source": [
        "def MLRBA_V1(ticker, covariances, returns, num_iterations=None, risk_free_rate = 0, \n",
        "             return_power = 1, std_power = 1, return_weight=1/3, corr_weight=1/3, vol_weight= 1/3, num_assets = 8):\n",
        "    \n",
        "    if num_iterations is None:\n",
        "        num_iterations = min(math.comb(len(ticker), num_assets), 100000)\n",
        "        \n",
        "    base_portfolio = np.random.choice(list(ticker), num_assets, replace=False)\n",
        "    #base_portfolio = list(ticker)[:num_assets]\n",
        "    highest_weighted_sharpe = -np.inf\n",
        "\n",
        "    all_portfolios = []\n",
        "\n",
        "    tested_assets = set()\n",
        "    best_iteration = 0\n",
        "\n",
        "    def _get_portfolio_stats (portfolio_assets, risk_free_rate = 0):\n",
        "        p_asset_ret = returns.loc[portfolio_assets].values\n",
        "        p_asset_var = covariances.loc[portfolio_assets, portfolio_assets].values\n",
        "        best_p_weights = maximize_sharpe(p_asset_ret, p_asset_var)\n",
        "        p_ret = np.dot(best_p_weights,p_asset_ret)\n",
        "        p_var = np.dot(best_p_weights, p_asset_var @ best_p_weights)\n",
        "        sharpe = get_sharpe_ratio(p_ret, p_var, risk_free_rate, return_power, std_power)\n",
        "\n",
        "        return p_asset_ret, p_asset_var, sharpe, p_ret, p_var, best_p_weights\n",
        "\n",
        "    def _update_portfolios_array(portfolios, assets, weights, p_ret, p_var):\n",
        "        portfolios.append({\n",
        "            \"tickers\": assets,\n",
        "            \"weights\": weights,\n",
        "            \"return\": p_ret,\n",
        "            \"variance\": p_var,\n",
        "            \"sharpe\": (p_ret-risk_free_rate)/np.sqrt(p_var),\n",
        "        })\n",
        "\n",
        "    curr_ret, curr_var, curr_weighted_sharpe, curr_p_return, curr_p_variance, curr_p_weights = _get_portfolio_stats(base_portfolio, risk_free_rate)\n",
        "    _update_portfolios_array(all_portfolios, base_portfolio, curr_p_weights, curr_p_return, curr_p_variance)\n",
        "\n",
        "    good_portfolios = all_portfolios.copy()\n",
        "    best_portfolio = base_portfolio.copy()\n",
        "\n",
        "    highest_weighted_sharpe = curr_weighted_sharpe\n",
        "    for i in tqdm(range(num_iterations)):\n",
        "        asset_to_remove = find_best_asset_to_remove(best_portfolio, curr_var, curr_ret)     #most_correlated_asset, _, _ = find_correlation_matrix(portfolio, curr_variances)\n",
        "        new_portfolio = [str(asset) for asset in best_portfolio if asset != asset_to_remove]\n",
        "\n",
        "        ranked_assets = find_asset_to_add(new_portfolio, ticker, covariances, returns, return_weight, corr_weight, vol_weight)         # Find the next best asset to add to the portfolio\n",
        "        asset_to_add = ranked_assets.index[0]\n",
        "\n",
        "        for asset in ranked_assets.index:\n",
        "            if asset not in tested_assets:\n",
        "                asset_to_add = asset\n",
        "                break\n",
        "\n",
        "        new_portfolio.append(asset_to_add)\n",
        "        tested_assets.add(asset_to_add)\n",
        "\n",
        "        if len(tested_assets) >= len(ticker) - num_assets:\n",
        "            print(\"All assets have been tested\")\n",
        "            break\n",
        "\n",
        "        # Substitute in and measure portfolio performance based on sharpe ratio\n",
        "        new_returns, new_var, new_weighted_sharpe, new_p_return, new_p_variance, new_p_weights = _get_portfolio_stats(new_portfolio, risk_free_rate)\n",
        "\n",
        "        _update_portfolios_array(all_portfolios, new_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
        "\n",
        "        if new_weighted_sharpe > highest_weighted_sharpe:\n",
        "            highest_weighted_sharpe = new_weighted_sharpe\n",
        "            best_portfolio = new_portfolio\n",
        "            curr_ret, curr_var = new_returns, new_var\n",
        "            best_iteration = i  # Update the best iteration\n",
        "\n",
        "            _update_portfolios_array(good_portfolios, new_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
        "\n",
        "            tested_assets.clear()\n",
        "\n",
        "        # If Sharpe ratio was worse, then move on to the next least correlated asset\n",
        "        # If Sharpe ratio is better, set as new base portfolio, and repeat the process for num_iterations times\n",
        "        # Adjust the sharpe ratio, maybe more emphasis on returns/volatility\n",
        "        # Update weights to value return or corr\n",
        "        # See how many iterations it takes to get here, whats a good threshold/stopping point\n",
        "        # Backtesting\n",
        "        # Train a model to maybe predict the sharpe ratio of a portfolio\n",
        "\n",
        "    base_details = good_portfolios[0]\n",
        "    best_details = good_portfolios[-1]\n",
        "\n",
        "    return base_details, best_details, good_portfolios, all_portfolios, best_iteration  \n",
        "\n",
        "base_portfolio, best_portfolio, good_portfolios, total_portfolios, best_iteration = MLRBA_V1(names, cov, annualized_returns)\n",
        "base_portfolio, best_portfolio, len(total_portfolios), best_iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Znh8vrYQabDW",
        "outputId": "f770bd56-eace-405c-a8c0-a1d835201f64"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[p[\"variance\"]**0.5 for p in total_portfolios],  # Convert variance to volatility\n",
        "    y=[p[\"return\"] for p in total_portfolios],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color=[p[\"sharpe\"] for p in total_portfolios],  # Sharpe Ratio\n",
        "        showscale=True,\n",
        "        size=7,\n",
        "        line=dict(width=1),\n",
        "        colorscale=\"RdBu\",\n",
        "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
        "    ),\n",
        "    hoverinfo='text',\n",
        "    text=[\n",
        "        f\"Return: {p['return']:.3%}<br>Volatility: {p['variance']**0.5:.3f}<br>\" +\n",
        "        f\"Sharpe Ratio: {p['return'] / (p['variance']**0.5):.3f}<br>\" +\n",
        "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
        "        for p in total_portfolios\n",
        "    ],\n",
        "    name=\"Portfolios\"\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=volatility,\n",
        "    y=annualized_returns,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    hovertext=[\n",
        "        f\"{name} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
        "        for name, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
        "    ],\n",
        "    marker=dict(\n",
        "        color='brown',\n",
        "        size=5,\n",
        "        symbol='triangle-up',  # Sets the marker shape to a triangle\n",
        "        line=dict(width=1)\n",
        "    ),\n",
        "    name=\"Individual Assets\"\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Sample of Random Portfolios',\n",
        "    xaxis_title='Volatility (Standard Deviation)',\n",
        "    yaxis_title='Annualized Return',\n",
        "    legend=dict(y=5\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fcrLEvL3abDW",
        "outputId": "7123cd39-a752-4640-be20-6fe3fcbc6c17"
      },
      "outputs": [],
      "source": [
        "sharpe_ratios = [portfolio['sharpe'] for portfolio in total_portfolios]\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=list(range(len(sharpe_ratios))), y=sharpe_ratios, mode='lines+markers'))\n",
        "fig.update_layout(title='Sharpe Ratio Over Iterations',\n",
        "                  xaxis_title='Iteration',\n",
        "                  yaxis_title='Sharpe Ratio',\n",
        "                  )\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Reinforcement Weight Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLRBA_V2(ticker, covariances, returns, num_iterations=None, risk_free_rate = 0, \n",
        "             return_power = 1, std_power = 1, return_weight=1/3, corr_weight=1/3, vol_weight= 1/3, num_assets = 8):\n",
        "    \n",
        "    if num_iterations is None:\n",
        "        num_iterations = min(math.comb(len(ticker), num_assets), 100000)\n",
        "\n",
        "    base_portfolio = np.random.choice(list(ticker), num_assets, replace=False)\n",
        "    #base_portfolio = list(ticker)[:num_assets]\n",
        "    highest_weighted_sharpe = -np.inf\n",
        "    all_portfolios = []\n",
        "    tested_assets = set()\n",
        "    best_iteration = 0\n",
        "\n",
        "    learning_rate = 0.05\n",
        "\n",
        "    def _get_portfolio_stats(portfolio_assets, risk_free_rate=0):\n",
        "        p_asset_ret = returns.loc[portfolio_assets].values\n",
        "        p_asset_var = covariances.loc[portfolio_assets, portfolio_assets].values\n",
        "        best_p_weights = maximize_sharpe(p_asset_ret, p_asset_var)\n",
        "        p_ret = np.dot(best_p_weights, p_asset_ret)\n",
        "        p_var = np.dot(best_p_weights, p_asset_var @ best_p_weights)\n",
        "        sharpe = get_sharpe_ratio(p_ret, p_var, risk_free_rate, return_power, std_power)\n",
        "        return p_asset_ret, p_asset_var, sharpe, p_ret, p_var, best_p_weights\n",
        "\n",
        "    def _update_portfolios_array(portfolios, assets, weights, p_ret, p_var):\n",
        "        portfolios.append({\n",
        "            \"tickers\": assets,\n",
        "            \"weights\": weights,\n",
        "            \"return\": p_ret,\n",
        "            \"variance\": p_var,\n",
        "            \"sharpe\": (p_ret - risk_free_rate) / np.sqrt(p_var),\n",
        "        })\n",
        "\n",
        "    curr_ret, curr_var, curr_weighted_sharpe, curr_p_return, curr_p_variance, curr_p_weights = _get_portfolio_stats(base_portfolio, risk_free_rate)\n",
        "    _update_portfolios_array(all_portfolios, base_portfolio, curr_p_weights, curr_p_return, curr_p_variance)\n",
        "\n",
        "    good_portfolios = all_portfolios.copy()\n",
        "    best_portfolio = base_portfolio.copy()\n",
        "    highest_weighted_sharpe = curr_weighted_sharpe\n",
        "\n",
        "    improvement_threshold = 0.001\n",
        "\n",
        "    for i in tqdm(range(num_iterations)):\n",
        "        asset_to_remove = find_best_asset_to_remove(best_portfolio, curr_var, curr_ret)\n",
        "        new_portfolio = [str(asset) for asset in best_portfolio if asset != asset_to_remove]\n",
        "\n",
        "        ranked_assets = find_asset_to_add(new_portfolio, ticker, covariances, returns, return_weight, corr_weight, vol_weight)\n",
        "        asset_to_add = ranked_assets.index[0]\n",
        "\n",
        "        for asset in ranked_assets.index:\n",
        "            if asset not in tested_assets:\n",
        "                asset_to_add = asset\n",
        "                break\n",
        "\n",
        "        new_portfolio.append(asset_to_add)\n",
        "        tested_assets.add(asset_to_add)\n",
        "\n",
        "        if len(tested_assets) >= len(ticker) - num_assets:\n",
        "            print(\"All assets have been tested\")\n",
        "            break\n",
        "\n",
        "        new_returns, new_var, new_weighted_sharpe, new_p_return, new_p_variance, new_p_weights = _get_portfolio_stats(new_portfolio, risk_free_rate)\n",
        "        _update_portfolios_array(all_portfolios, new_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
        "\n",
        "        if new_weighted_sharpe > highest_weighted_sharpe:\n",
        "            improvement = new_weighted_sharpe - highest_weighted_sharpe\n",
        "            highest_weighted_sharpe = new_weighted_sharpe\n",
        "            best_portfolio = new_portfolio\n",
        "            curr_ret, curr_var = new_returns, new_var\n",
        "            best_iteration = i\n",
        "\n",
        "            asset_return = returns.loc[asset_to_add]\n",
        "            asset_vol = np.sqrt(covariances.loc[asset_to_add, asset_to_add])\n",
        "            avg_return = returns.mean()\n",
        "            avg_vol = np.sqrt(np.diag(covariances)).mean()\n",
        "\n",
        "            corr_with_portfolio = correlation_matrix.loc[new_portfolio, asset_to_add].drop(asset_to_add).mean()\n",
        "            avg_corr_in_portfolio = correlation_matrix.loc[new_portfolio].drop(asset_to_add, axis=1).mean().mean()\n",
        "            \n",
        "            # Update weights using the current learning rate\n",
        "            if asset_return > avg_return:\n",
        "                return_weight += learning_rate * (asset_return - avg_return) / avg_return\n",
        "            else:\n",
        "                return_weight -= learning_rate * (avg_return - asset_return) / avg_return\n",
        "\n",
        "            if asset_vol < avg_vol:\n",
        "                vol_weight -= learning_rate * (avg_vol - asset_vol) / avg_vol\n",
        "            else:\n",
        "                vol_weight += learning_rate * (asset_vol - avg_vol) / avg_vol\n",
        "\n",
        "            if corr_with_portfolio < avg_corr_in_portfolio:\n",
        "                corr_weight += learning_rate * (avg_corr_in_portfolio - corr_with_portfolio) / avg_corr_in_portfolio\n",
        "            else:\n",
        "                corr_weight -= learning_rate * (corr_with_portfolio - avg_corr_in_portfolio) / avg_corr_in_portfolio\n",
        "\n",
        "            total = return_weight + corr_weight + vol_weight\n",
        "            return_weight /= total\n",
        "            corr_weight /= total\n",
        "            vol_weight /= total\n",
        "\n",
        "            if improvement < improvement_threshold:\n",
        "                learning_rate *= 0.95\n",
        "            else:\n",
        "                learning_rate *= 1.01\n",
        "\n",
        "            _update_portfolios_array(good_portfolios, new_portfolio, new_p_weights, new_p_return, new_p_variance)\n",
        "            tested_assets.clear()\n",
        "\n",
        "    base_details = good_portfolios[0]\n",
        "    best_details = good_portfolios[-1]\n",
        "\n",
        "    print(\"Final Weights:\", return_weight, corr_weight, vol_weight)\n",
        "    \n",
        "    return base_details, best_details, good_portfolios, all_portfolios, best_iteration\n",
        "\n",
        "\n",
        "base_portfolio, best_portfolio, good_portfolios, total_portfolios, best_iteration = MLRBA_V2(names, cov, annualized_returns)\n",
        "best_portfolio, best_portfolio['sharpe'], best_iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[p[\"variance\"]**0.5 for p in total_portfolios],  # Convert variance to volatility\n",
        "    y=[p[\"return\"] for p in total_portfolios],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color=[p[\"sharpe\"] for p in total_portfolios],  # Sharpe Ratio\n",
        "        showscale=True,\n",
        "        size=7,\n",
        "        line=dict(width=1),\n",
        "        colorscale=\"RdBu\",\n",
        "        colorbar=dict(title=\"Sharpe<br>Ratio\")\n",
        "    ),\n",
        "    hoverinfo='text',\n",
        "    text=[\n",
        "        f\"Return: {p['return']:.3%}<br>Volatility: {p['variance']**0.5:.3f}<br>\" +\n",
        "        f\"Sharpe Ratio: {p['return'] / (p['variance']**0.5):.3f}<br>\" +\n",
        "        \"<br>\".join([f\"{p['tickers'][i]}: Weight={p['weights'][i]:.3f}\" for i in range(len(p['tickers']))])\n",
        "        for p in total_portfolios\n",
        "    ],\n",
        "    name=\"Portfolios\"\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=volatility,\n",
        "    y=annualized_returns,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    hovertext=[\n",
        "        f\"{name} <br>Volatility: {vol:.3f} <br>Returns: {ret:.3%} <br>Sharpe Ratio: {sr:.3f}\"\n",
        "        for name, vol, ret, sr in zip(names, volatility, annualized_returns, sharpe_ratios)\n",
        "    ],\n",
        "    marker=dict(\n",
        "        color='brown',\n",
        "        size=5,\n",
        "        symbol='triangle-up',  # Sets the marker shape to a triangle\n",
        "        line=dict(width=1)\n",
        "    ),\n",
        "    name=\"Individual Assets\"\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Sample of Random Portfolios',\n",
        "    xaxis_title='Volatility (Standard Deviation)',\n",
        "    yaxis_title='Annualized Return',\n",
        "    legend=dict(y=5\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sharpe_ratios = [portfolio['sharpe'] for portfolio in total_portfolios]\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=list(range(len(sharpe_ratios))), y=sharpe_ratios, mode='lines+markers'))\n",
        "fig.update_layout(title='Sharpe Ratio Over Iterations',\n",
        "                  xaxis_title='Iteration',\n",
        "                  yaxis_title='Sharpe Ratio',\n",
        "                  )\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_MLRBA_multiple_times(names, cov, annualized_returns, num_runs, version):\n",
        "    total_good_portfolios_length = 0\n",
        "    best_portfolios = []\n",
        "    best_iterations = []\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        if version == 'v1':\n",
        "            _, best_portfolio, good_portfolios, _, best_iteration = MLRBA_V1(names, cov, annualized_returns)\n",
        "        elif version == 'v2':\n",
        "            _, best_portfolio, good_portfolios, _, best_iteration = MLRBA_V2(names, cov, annualized_returns)\n",
        "        \n",
        "        total_good_portfolios_length += len(good_portfolios)\n",
        "        best_portfolios.append(best_portfolio)\n",
        "        best_iterations.append(best_iteration)\n",
        "\n",
        "    average_length = total_good_portfolios_length / num_runs\n",
        "    average_iteration = statistics.fmean(best_iterations)\n",
        "    std_dev_iteration = statistics.stdev(best_iterations) if num_runs > 1 else 0\n",
        "\n",
        "    return average_length, best_portfolios, average_iteration, std_dev_iteration, best_iterations\n",
        "\n",
        "num_runs = 1\n",
        "average_good_portfolios_length_V1, _, average_iteration_V1, std_dev_V1, best_iterations_v1 = run_MLRBA_multiple_times(names, cov, annualized_returns, num_runs, 'v1')\n",
        "print(\"Average length of good portfolios:\", average_good_portfolios_length_V1, \"Average iterations:\", average_iteration_V1, \"Std Dev:\", std_dev_V1)\n",
        "\n",
        "average_good_portfolios_length_V2, _, average_iteration_V2, std_dev_V2, best_iterations_v2 = run_MLRBA_multiple_times(names, cov, annualized_returns, num_runs, 'v2')\n",
        "print(\"Average length of good portfolios:\", average_good_portfolios_length_V2, \"Average iterations:\", average_iteration_V2, \"Std Dev:\", std_dev_V2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=[\n",
        "        \"Iterations to Reach Best Portfolio (Line Plot)\",\n",
        "        \"Iterations to Reach Best Portfolio (Bar Plot)\"\n",
        "    ],)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(1, num_runs + 1)), y=best_iterations_v1, mode='lines', name='MLRBA V1'),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(1, num_runs + 1)), y=best_iterations_v2, mode='lines', name='MLRBA V2'),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.update_xaxes(title_text='Run Number', row=1, col=1)\n",
        "fig.update_yaxes(title_text='Average Number of Iterations to Best Portfolio', row=1, col=1)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(x=['MLRBA V1'], y=[average_iteration_V1], name='MLRBA V1', width=0.4,\n",
        "           error_y=dict(type='data', array=[std_dev_V1], visible=True)),\n",
        "    row=1, col=2\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Bar(x=['MLRBA V2'], y=[average_iteration_V2], name='MLRBA V2', width=0.4,\n",
        "           error_y=dict(type='data', array=[std_dev_V2], visible=True)),\n",
        "    row=1, col=2\n",
        ")\n",
        "fig.update_xaxes(title_text='Run Number', row=1, col=2)\n",
        "fig.update_yaxes(title_text='Average Number of Iterations to Best Portfolio', row=1, col=2)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.0 Portfolio Prediction using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PortfolioPredictor:\n",
        "    def __init__(self, raw_data_train, raw_data_test, best_portfolio, n_steps=5, epochs=50, batch_size=32):\n",
        "        self.raw_data_train = raw_data_train\n",
        "        self.raw_data_test = raw_data_test\n",
        "        self.best_portfolio = best_portfolio\n",
        "        self.n_steps = n_steps\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Selecting data for the tickers\n",
        "        best_portfolio_data_train = self.raw_data_train[self.best_portfolio['tickers']]\n",
        "        best_portfolio_data_test = self.raw_data_test[self.best_portfolio['tickers']]\n",
        "        weights = np.array(self.best_portfolio['weights'])\n",
        "\n",
        "        # Scaling the data\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        normalized_train_data = scaler.fit_transform(best_portfolio_data_train)\n",
        "        normalized_test_data = scaler.transform(best_portfolio_data_test)\n",
        "\n",
        "        # Calculating weighted returns\n",
        "        self.weighted_returns_train = np.dot(normalized_train_data, weights)\n",
        "        self.weighted_returns_test = np.dot(normalized_test_data, weights)\n",
        "\n",
        "    def create_datasets(self, data):\n",
        "        X, y = [], []\n",
        "        for i in range(self.n_steps, len(data)):\n",
        "            X.append(data[i - self.n_steps:i, :])\n",
        "            y.append(data[i, :])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def build_model(self):\n",
        "        self.model = Sequential([\n",
        "            LSTM(50, activation='relu', return_sequences=True),\n",
        "            Dropout(0.2),\n",
        "            LSTM(50, activation='relu', return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(1),\n",
        "        ])\n",
        "\n",
        "        def tf_weighted_mse(y_true, y_pred, alpha=0.3):\n",
        "            weights = tf.exp(alpha * tf.range(tf.shape(y_true)[0], dtype=tf.float32))\n",
        "            weights /= tf.reduce_sum(weights)\n",
        "            squared_errors = tf.square(y_true - y_pred)\n",
        "            weighted_squared_errors = weights * squared_errors\n",
        "            return tf.reduce_mean(weighted_squared_errors)\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss=tf_weighted_mse)\n",
        "\n",
        "    def train_model(self):\n",
        "        X_train_weighted, y_train_weighted = self.create_datasets(self.weighted_returns_train.reshape(-1, 1))\n",
        "        self.history = self.model.fit(X_train_weighted, y_train_weighted, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1, verbose=0)\n",
        "\n",
        "    def predict(self, predict_days=None):\n",
        "        # Generate test set data\n",
        "        X_test_weighted, y_test_weighted = self.create_datasets(self.weighted_returns_test.reshape(-1, 1))\n",
        "        \n",
        "        # Determine the number of predictions based on predict_days\n",
        "        if predict_days is not None:\n",
        "            # Ensure that predict_days does not exceed the length of X_test_weighted\n",
        "            predict_days = min(predict_days, len(X_test_weighted))\n",
        "            # Slice the last 'predict_days' entries for prediction\n",
        "            X_test_to_predict = X_test_weighted[-predict_days:]\n",
        "            self.predictions = self.model.predict(X_test_to_predict)\n",
        "            self.y_test_weighted = y_test_weighted[-predict_days:]\n",
        "        else:\n",
        "            self.predictions = self.model.predict(X_test_weighted)\n",
        "            self.y_test_weighted = y_test_weighted\n",
        "        \n",
        "        # Normalize and compute cumulative returns for the predicted period\n",
        "        return self.predictions\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x=np.arange(1, len(self.history.history['loss'])+1), y=self.history.history['loss'], mode='lines', name='Training Loss'))\n",
        "        fig.add_trace(go.Scatter(x=np.arange(1, len(self.history.history['val_loss'])+1), y=self.history.history['val_loss'], mode='lines', name='Validation Loss'))\n",
        "        fig.update_layout(title='Training and Validation Loss Over Epochs',\n",
        "                          xaxis_title='Epoch',\n",
        "                          yaxis_title='Loss',\n",
        "                          legend_title='Type of Loss')\n",
        "        fig.show()\n",
        "\n",
        "    def plot_predictions(self):\n",
        "        normalized_test = self.normalize_cumulative_returns(self.y_test_weighted)\n",
        "        normalized_predicted = self.normalize_cumulative_returns(self.predictions)\n",
        "\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x=self.raw_data_test.index[-len(normalized_test):], y=normalized_test, mode='lines', name='Actual Weighted Returns'))\n",
        "        fig.add_trace(go.Scatter(x=self.raw_data_test.index[-len(normalized_predicted):], y=normalized_predicted, mode='lines', name='Predicted Weighted Returns'))\n",
        "        fig.update_layout(title='Actual vs Predicted Weighted Portfolio Returns',\n",
        "                          xaxis_title='Date',\n",
        "                          yaxis_title='Weighted Returns',\n",
        "                          legend_title='Type')\n",
        "        fig.show()\n",
        "\n",
        "    def normalize_cumulative_returns(self, data):\n",
        "        data_series = pd.Series(data.flatten())\n",
        "        percentage_changes = data_series.pct_change()\n",
        "        cumulative_returns = (1 + percentage_changes).cumprod()\n",
        "        start_value = cumulative_returns.iloc[1]\n",
        "        normalized_returns = (cumulative_returns / start_value) * 100\n",
        "        return normalized_returns\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "portfolio_predictor = PortfolioPredictor(raw_data_train, raw_data_test, best_portfolio, n_steps=3)\n",
        "\n",
        "portfolio_predictor.preprocess_data()\n",
        "portfolio_predictor.build_model()\n",
        "portfolio_predictor.train_model()\n",
        "prediction = portfolio_predictor.predict()   \n",
        "portfolio_predictor.plot_loss()\n",
        "portfolio_predictor.plot_predictions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_portfolios_over_time(raw_data, window_size=5, num_windows=None, threshold=0.05, epochs=10):\n",
        "    split = len(raw_data.index) // 2\n",
        "    all_good_portfolios = []\n",
        "    if num_windows is None:\n",
        "        num_windows = split // window_size\n",
        "    \n",
        "    previous_best_portfolio = None \n",
        "\n",
        "    for i in range(num_windows):\n",
        "        curr_split = i * window_size\n",
        "\n",
        "        loop_raw_data_train = raw_data.iloc[:split + curr_split]\n",
        "        loop_raw_data_test = raw_data.iloc[split + curr_split:]\n",
        "        \n",
        "        loop_names, loop_annualized_returns, loop_cov, _ = get_matrices(loop_raw_data_train)\n",
        "        \n",
        "        _, loop_best_portfolio, loop_good_portfolios, _, _ = MLRBA_V2(loop_names, loop_cov, loop_annualized_returns)\n",
        "        best_sharpe = loop_best_portfolio['sharpe']\n",
        "        \n",
        "        close_to_best = []\n",
        "        for j in range(len(loop_good_portfolios)):\n",
        "            difference = abs((best_sharpe - loop_good_portfolios[j]['sharpe']) / best_sharpe)\n",
        "            if difference < threshold:\n",
        "                close_to_best.append(loop_good_portfolios[j])\n",
        "\n",
        "        if previous_best_portfolio is not None:\n",
        "            close_to_best.append(previous_best_portfolio)\n",
        "\n",
        "        portfolio_results = {}\n",
        "        for id, portfolio in enumerate(close_to_best):\n",
        "            portfolio_predictor = PortfolioPredictor(loop_raw_data_train, loop_raw_data_test, portfolio, n_steps=window_size, epochs=epochs)\n",
        "            portfolio_predictor.preprocess_data()\n",
        "            portfolio_predictor.build_model()\n",
        "            portfolio_predictor.train_model()\n",
        "            prediction = portfolio_predictor.predict(window_size) \n",
        "\n",
        "            percentage_diff = (prediction[window_size-1] - prediction[0]) / prediction[0]\n",
        "            portfolio_results[id] = percentage_diff\n",
        "\n",
        "        print(portfolio_results)          \n",
        "        best_id = max(portfolio_results, key=portfolio_results.get)\n",
        "        predicted_best_portfolio = close_to_best[best_id]\n",
        "\n",
        "        previous_best_portfolio = predicted_best_portfolio\n",
        "        \n",
        "        start_date = loop_raw_data_test.index[0]\n",
        "        end_date = loop_raw_data_test.index[window_size-1]\n",
        "        \n",
        "        all_good_portfolios.append({\n",
        "            \"portfolio\": predicted_best_portfolio,\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date\n",
        "        })\n",
        "        print('Current iteration:' + i + ', the best portfolio found was portfolio:' + best_id)\n",
        "    \n",
        "    return all_good_portfolios\n",
        "\n",
        "all_good_portfolios = evaluate_portfolios_over_time(raw_data, window_size=3, num_windows=5, threshold=0.05, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_asset_returns(raw_data, assets, start_date, end_date):\n",
        "    if not isinstance(raw_data.index, pd.DatetimeIndex):\n",
        "        raw_data.index = pd.to_datetime(raw_data.index)\n",
        "\n",
        "    filtered_data = raw_data.loc[start_date:end_date, assets]\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "ML_portfolio = []\n",
        "\n",
        "for i in range(len(all_good_portfolios)):\n",
        "    curr_best_portfolio = all_good_portfolios[i]['portfolio']\n",
        "    best_curr_port_assets = curr_best_portfolio['tickers']\n",
        "\n",
        "    best_curr_port_assets_test_data = extract_asset_returns(raw_data, best_curr_port_assets, all_good_portfolios[i]['start_date'], all_good_portfolios[i]['end_date'])\n",
        "\n",
        "    curr_best_portfolio_weights = curr_best_portfolio['weights']\n",
        "\n",
        "    weighted_returns = best_curr_port_assets_test_data.mul(curr_best_portfolio_weights, axis='columns')\n",
        "    portfolio_daily_returns = weighted_returns.sum(axis=1)\n",
        "\n",
        "    ML_portfolio.append(portfolio_daily_returns)\n",
        "\n",
        "ML_portfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def chain_portfolio_performance(weekly_series_list, starting_value=100):\n",
        "    continuous_series = pd.Series()\n",
        "    current_value = starting_value\n",
        "\n",
        "    for week_series in weekly_series_list:\n",
        "        # Normalize the week so that it starts at 1 (or current_value)\n",
        "        week_normalized = week_series / week_series.iloc[0]\n",
        "        # Scale the normalized week to start at current_value\n",
        "        week_scaled = week_normalized * current_value\n",
        "        # Update the current_value to the last value of this week\n",
        "        current_value = week_scaled.iloc[-1]\n",
        "        # Append the week_series to the continuous_series\n",
        "        continuous_series = pd.concat([continuous_series, week_scaled])\n",
        "    \n",
        "    return continuous_series\n",
        "\n",
        "ML_portfolio_streamed = chain_portfolio_performance(ML_portfolio, starting_value=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ML_daily_returns = ML_portfolio_streamed.pct_change()\n",
        "ML_cumulative_returns = (1 + ML_daily_returns).cumprod()\n",
        "\n",
        "ML_cumulative_returns.iloc[0] = 1\n",
        "ML_portfolio_normalized = (ML_cumulative_returns / ML_cumulative_returns.iloc[0]) * 100\n",
        "\n",
        "Nasdaq_comp = getNasdaq_comp(ML_portfolio_streamed.index[0], ML_portfolio_streamed.index[-1])\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=Nasdaq_comp.index,\n",
        "    y=Nasdaq_comp['Normalized'],\n",
        "    mode='lines',\n",
        "    name='Nasdaq Composite'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=ML_cumulative_returns.index,\n",
        "    y=ML_portfolio_normalized,\n",
        "    mode='lines',\n",
        "    name='Portfolio Growth'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Comparison of Portfolio vs. Nasdaq Composite Growth',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Normalized Value (Base 100%)',\n",
        "    xaxis=dict(\n",
        "        type='date',\n",
        "        tickformat='%b %Y',\n",
        "        tickmode='auto'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFtx1nkbabDW"
      },
      "source": [
        "## 7.0 Find Optimal Portfolio Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSnuoMXUabDW",
        "outputId": "821dc1cf-0262-45fb-9296-ae3847b54dce"
      },
      "outputs": [],
      "source": [
        "all_portfolios, dominant_portfolios = MonteCarloRBA(names, cov, annualized_returns, 10000, \"random\", 3, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pXRrClKnabDW",
        "outputId": "c0cb593c-3e56-4060-e1d1-a69c2221d9e6"
      },
      "outputs": [],
      "source": [
        "rd_portfolio_sizes = [len(portfolio['tickers']) for portfolio in all_portfolios]\n",
        "rd_volatility = [np.sqrt(portfolio['variance']) for portfolio in all_portfolios]\n",
        "rd_returns = [portfolio['return'] for portfolio in all_portfolios]\n",
        "\n",
        "volatility_by_size = defaultdict(list)\n",
        "for size, vol, ret in zip(rd_portfolio_sizes, rd_volatility, rd_returns):\n",
        "    volatility_by_size[size].append((vol, ret))\n",
        "\n",
        "average_volatility = {size: np.mean([v[0] for v in vols]) for size, vols in volatility_by_size.items()}\n",
        "average_returns = {size: np.mean([v[1] for v in vols]) for size, vols in volatility_by_size.items()}\n",
        "\n",
        "sorted_sizes = sorted(average_volatility.keys())\n",
        "sorted_average_vols = [average_volatility[size] for size in sorted_sizes]\n",
        "sorted_average_rets = [average_returns[size] for size in sorted_sizes]\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=sorted_sizes,\n",
        "    y=sorted_average_vols,\n",
        "    mode='lines',\n",
        "    name='Average Volatility'\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=sorted_sizes,\n",
        "    y=sorted_average_rets,\n",
        "    mode='lines',\n",
        "    name='Average Returns'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Average Volatility and Returns by Portfolio Size',\n",
        "    xaxis_title='Number of Assets in Portfolio',\n",
        "    yaxis_title='Average Value',\n",
        "    xaxis=dict(type='category'),\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5u_KAQS1CRr"
      },
      "source": [
        "## 8.0 Testing Against Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "n4AB0laC1CRr",
        "outputId": "4b139451-2662-435b-dc3d-56a5eed5b189"
      },
      "outputs": [],
      "source": [
        "best_port_assets = best_portfolio['tickers']\n",
        "best_port_assets_test_data = raw_data_test.loc[:, best_port_assets]\n",
        "\n",
        "Nasdaq_comp = getNasdaq_comp(best_port_assets_test_data.index[0], best_port_assets_test_data.index[-1])\n",
        "\n",
        "best_portfolio_weights = best_portfolio['weights']\n",
        "daily_returns = best_port_assets_test_data.pct_change()\n",
        "weighted_returns = daily_returns.mul(best_portfolio_weights, axis='columns')\n",
        "portfolio_daily_returns = weighted_returns.sum(axis=1)\n",
        "portfolio_cumulative_returns = (1 + portfolio_daily_returns).cumprod()\n",
        "\n",
        "portfolio_start = portfolio_cumulative_returns.iloc[0]\n",
        "portfolio_normalized = (portfolio_cumulative_returns / portfolio_start) * 100\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=Nasdaq_comp.index,\n",
        "    y=Nasdaq_comp['Normalized'],\n",
        "    mode='lines',\n",
        "    name='Nasdaq Composite'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=portfolio_cumulative_returns.index,\n",
        "    y=portfolio_normalized,\n",
        "    mode='lines',\n",
        "    name='Portfolio Growth'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=ML_cumulative_returns.index,\n",
        "    y=ML_portfolio_normalized,\n",
        "    mode='lines',\n",
        "    name='Portfolio Growth'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Comparison of Portfolio vs. Nasdaq Composite Growth',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Normalized Value (Base 100)',\n",
        "    xaxis=dict(\n",
        "        type='date',\n",
        "        tickformat='%b %Y',\n",
        "        tickmode='auto'\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "roboA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
